{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675d500a",
   "metadata": {},
   "source": [
    "## Actions\n",
    "- Load seed data into DB. \n",
    "- Clean and transform scraped data\n",
    "- Create an idempotent etl job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5008c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c588289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8b61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\ai_tools_scraped.json\"\n",
    "seed_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\seeded_ai_agents.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a81ac",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30430eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(source_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Data Path\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises error for unsupported data type.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas Dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ext = Path(source_path).suffix\n",
    "        if ext == \".csv\":\n",
    "            return pd.read_csv(source_path)\n",
    "        elif ext == \".json\":\n",
    "            return pd.read_json(source_path)\n",
    "        elif ext == \".parquet\":\n",
    "            return pd.read_parquet(source_path)\n",
    "        logger.info(\"Data successfully read!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}. Unsupported file format! Use csv, json or parquet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a875aff",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69a1ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 379 entries, 0 to 378\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name         379 non-null    object\n",
      " 1   description  379 non-null    object\n",
      " 2   url          377 non-null    object\n",
      " 3   tags         378 non-null    object\n",
      " 4   pricing      378 non-null    object\n",
      " 5   page         379 non-null    int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 17.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72 entries, 0 to 71\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   name          72 non-null     object\n",
      " 1   description   72 non-null     object\n",
      " 2   homepage_url  72 non-null     object\n",
      " 3   category      72 non-null     object\n",
      " 4   source        72 non-null     object\n",
      " 5   created_at    72 non-null     object\n",
      " 6   updated_at    72 non-null     object\n",
      " 7   trending      72 non-null     object\n",
      "dtypes: object(8)\n",
      "memory usage: 4.6+ KB\n"
     ]
    }
   ],
   "source": [
    "scraped_df = read_data(scraped_data_source)\n",
    "seed_df = read_data(seed_data_source)\n",
    "\n",
    "scraped_df.info()\n",
    "seed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fa97d",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Cleaning \"tags\" column\n",
    "- separate the values in the list and choose the unique tag. \n",
    "- Each tag must be just a value. (i.e list of len 1)\n",
    "    + no # in value \n",
    "    + no duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b49919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = df.drop(columns=[col for col in [\"pricing\", \"page\"] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        logger.info(\n",
    "            \"Columns dropped and null values dropped.\",\n",
    "            extra={\n",
    "                \"Cols dropped\": [\"pricing\", \"page\"],\n",
    "                \"Null Values Dropped\": len(df) - len(new_df),\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error Raised {e}! Is the input a dataframe? Use a pandas dataframe.\",\n",
    "            exc_info=True,\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_hashtags(tags):\n",
    "    try:\n",
    "        if isinstance(tags, list):\n",
    "            clean = [tag for tag in tags if \"#\" not in tag]\n",
    "        elif isinstance(tags, str):\n",
    "            clean = [tags] if \"#\" not in tags else []\n",
    "        else:\n",
    "            clean = []\n",
    "        clean = \",\".join(clean)\n",
    "\n",
    "        if len(clean) < 4:\n",
    "            clean = clean.upper()\n",
    "        else:\n",
    "            clean = clean.lower().capitalize()\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error Raised at tags column cleaning {e}! Use tags column.\", exc_info=True\n",
    "        )\n",
    "    return clean\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    try:\n",
    "        # df = baseline_cleaning(df=scraped_df)\n",
    "        df = df.drop(columns=[col for col in [\"pricing\", \"page\"] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "        if \"tags\" in df.columns:\n",
    "            new_df[\"tags\"] = new_df[\"tags\"].apply(remove_hashtags)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        logger.info(\n",
    "            \"Columns dropped and null values dropped.\",\n",
    "            extra={\n",
    "                \"Cols dropped\": [\"pricing\", \"page\"],\n",
    "                \"Null Values Dropped\": len(df) - len(new_df),\n",
    "            },\n",
    "        )\n",
    "        logger.info(\"Tags Column Successfully cleaned.\")\n",
    "        logger.info(\"Data successfully cleaned!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at full cleaning process: {e}!\", exc_info=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfcfc1",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef8b05bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4102930063.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif df.\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def needs_transformation(df):\n",
    "    # Add your logic here (e.g., missing columns, data types)\n",
    "    # check for missing values,\n",
    "\n",
    "    if df.\n",
    "\n",
    "    return some_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3606b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_created_at(filepath: str) -> str:\n",
    "    try:\n",
    "        created_timestamp = os.path.getctime(filepath)\n",
    "        created_date = datetime.fromtimestamp(created_timestamp)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised: {e}!\", exc_info=True)\n",
    "    return created_date.strftime(\"%Y-%M-%d\")\n",
    "\n",
    "\n",
    "def transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        created_day = get_created_at(scraped_data_source)\n",
    "        df[\"source\"] = \"https://aitoolsdirectory.com/\"\n",
    "\n",
    "        if \"created_at\" in df.columns:\n",
    "            if df[\"created_at\"] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df[\"created_at\"] = created_day\n",
    "        else:\n",
    "            df[\"created_at\"] = created_day\n",
    "        if \"updated_at\" in df.columns:\n",
    "            if df[\"updated_at\"] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df[\"updated_at\"] = None\n",
    "        else:\n",
    "            df[\"updated_at\"] = None\n",
    "\n",
    "        if \"trending\" not in df.columns:\n",
    "            df[\"trending\"] = None\n",
    "        else:\n",
    "            df[\"trending\"] = df[\"trending\"].replace(\n",
    "                {\"Low\": False, \"Medium\": True, \"High\": True}\n",
    "            )\n",
    "\n",
    "        trans_df = df.rename(columns={\"url\": \"homepage_url\", \"tags\": \"category\"})\n",
    "\n",
    "        trans_df[\"created_at\"] = pd.to_datetime(\n",
    "            trans_df[\"created_at\"], format=\"%Y-%M-%d\", errors=\"coerce\"\n",
    "        )\n",
    "        trans_df[\"updated_at\"] = pd.to_datetime(\n",
    "            trans_df[\"updated_at\"], format=\"%Y-%M-%d\", errors=\"coerce\"\n",
    "        )\n",
    "        trans_df[\"trending\"] = trans_df[\"trending\"].notna().astype(bool)\n",
    "\n",
    "        logger.info(\"Data successfully transformed!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at transformation: {e}!\", exc_info=True)\n",
    "    return trans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f82458",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca5e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_et(source: str) -> pd.DataFrame:\n",
    "    # Extract\n",
    "    scraped_df = read_data(source)\n",
    "\n",
    "    # Clean\n",
    "    clean_df = clean_data(scraped_df)\n",
    "\n",
    "    # Transform\n",
    "    trans_df = transform_data(clean_df)\n",
    "\n",
    "    # Load\n",
    "    \"I dey come. Mapami\"\n",
    "\n",
    "    return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e022e0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 12:22:48,763 - INFO - Columns dropped and null values dropped.\n",
      "2025-05-28 12:22:48,771 - INFO - Tags Column Successfully cleaned.\n",
      "2025-05-28 12:22:48,772 - INFO - Data successfully cleaned!\n",
      "C:\\Users\\APIN-PC\\AppData\\Local\\Temp\\ipykernel_24432\\3309546027.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['trending'] = df['trending'].replace({\"Low\": False, \"Medium\": True, \"High\": True})\n",
      "2025-05-28 12:22:48,879 - INFO - Data successfully transformed!\n",
      "2025-05-28 12:22:48,955 - INFO - Columns dropped and null values dropped.\n",
      "2025-05-28 12:22:48,958 - INFO - Tags Column Successfully cleaned.\n",
      "2025-05-28 12:22:48,960 - INFO - Data successfully cleaned!\n",
      "2025-05-28 12:22:48,989 - INFO - Data successfully transformed!\n"
     ]
    }
   ],
   "source": [
    "seed_clean_df = run_basic_et(source=seed_data_source)\n",
    "scraped_clean_df = run_basic_et(source=scraped_data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890b3c8",
   "metadata": {},
   "source": [
    "## Merging Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.merge(seed_clean_df, scraped_clean_df, how=\"outer\")\n",
    "comp_df.drop_duplicates(subset=\"name\", inplace=True)\n",
    "comp_df = comp_df.reset_index(drop=True)\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9d30b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_list = pd.Series(comp_df[\"name\"])\n",
    "ai_list.to_csv(\"../etl/data/Ai_tools_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5aa6bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>homepage_url</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Grammarly</td>\n",
       "      <td>AI writing assistance and grammar checking</td>\n",
       "      <td>https://www.grammarly.com/</td>\n",
       "      <td>Productivity</td>\n",
       "      <td>https://aitoolsdirectory.com/</td>\n",
       "      <td>2009-01-01 00:01:00</td>\n",
       "      <td>2024-01-01 00:10:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Grammarly</td>\n",
       "      <td>Grammarly’s powerful AI features can superchar...</td>\n",
       "      <td>https://link.aitoolsdirectory.com/grammarly</td>\n",
       "      <td>Productivity</td>\n",
       "      <td>https://aitoolsdirectory.com/</td>\n",
       "      <td>2025-01-25 00:23:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name                                        description  \\\n",
       "150  Grammarly         AI writing assistance and grammar checking   \n",
       "151  Grammarly  Grammarly’s powerful AI features can superchar...   \n",
       "\n",
       "                                    homepage_url      category  \\\n",
       "150                   https://www.grammarly.com/  Productivity   \n",
       "151  https://link.aitoolsdirectory.com/grammarly  Productivity   \n",
       "\n",
       "                            source          created_at          updated_at  \\\n",
       "150  https://aitoolsdirectory.com/ 2009-01-01 00:01:00 2024-01-01 00:10:00   \n",
       "151  https://aitoolsdirectory.com/ 2025-01-25 00:23:00                 NaT   \n",
       "\n",
       "     trending  \n",
       "150      True  \n",
       "151     False  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_df[comp_df[\"name\"] == \"Grammarly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d4b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                    object\n",
       "description             object\n",
       "homepage_url            object\n",
       "category                object\n",
       "source                  object\n",
       "created_at      datetime64[ns]\n",
       "updated_at      datetime64[ns]\n",
       "trending                  bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_clean_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052694c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name              0\n",
       "description       0\n",
       "homepage_url      0\n",
       "category          0\n",
       "source            0\n",
       "created_at        0\n",
       "updated_at      376\n",
       "trending          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_clean_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d847e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 14:28:16,105 - INFO - Columns with missing values present updated_at\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "description\n",
      "homepage_url\n",
      "category\n",
      "source\n"
     ]
    }
   ],
   "source": [
    "scraped_df.isna().sum()\n",
    "\n",
    "for col in scraped_clean_df.columns:\n",
    "    if scraped_clean_df[col].isna().sum() > 0:\n",
    "        logger.info(\"Columns with missing values present %s\", col)\n",
    "    if scraped_clean_df[col].dtype == list:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f9466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name           object\n",
       "description    object\n",
       "url            object\n",
       "tags           object\n",
       "pricing        object\n",
       "page            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "607bc10d",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Pending tasks\n",
    "- Load data into db\n",
    "- Get created_at for scraped ai tools. \n",
    "\n",
    "### Action\n",
    "- Create a new csv file to combine both seeded and scraped tools.\n",
    "- Check for duplicates.\n",
    "- Log duplicates and update duplicates\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-directory-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
