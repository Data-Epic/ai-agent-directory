{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675d500a",
   "metadata": {},
   "source": [
    "## Actions\n",
    "- Load seed data into DB. \n",
    "- Clean and transform scraped data\n",
    "- Create an idempotent etl job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5008c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c588289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8b61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\ai_tools_scraped.json\"\n",
    "seed_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\seeded_ai_agents.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a81ac",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30430eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(source_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Data Path\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises error for unsupported data type.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas Dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ext = Path(source_path).suffix\n",
    "        if ext == \".csv\":\n",
    "            return pd.read_csv(source_path)\n",
    "        elif ext == \".json\":\n",
    "            return pd.read_json(source_path)\n",
    "        elif ext == \".parquet\":\n",
    "            return pd.read_parquet(source_path)\n",
    "        logger.info(\"Data successfully read!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}. Unsupported file format! Use csv, json or parquet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a875aff",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69a1ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 379 entries, 0 to 378\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name         379 non-null    object\n",
      " 1   description  379 non-null    object\n",
      " 2   url          377 non-null    object\n",
      " 3   tags         378 non-null    object\n",
      " 4   pricing      378 non-null    object\n",
      " 5   page         379 non-null    int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 17.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72 entries, 0 to 71\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   name          72 non-null     object\n",
      " 1   description   72 non-null     object\n",
      " 2   homepage_url  72 non-null     object\n",
      " 3   category      72 non-null     object\n",
      " 4   source        72 non-null     object\n",
      " 5   created_at    72 non-null     object\n",
      " 6   updated_at    72 non-null     object\n",
      " 7   trending      72 non-null     object\n",
      "dtypes: object(8)\n",
      "memory usage: 4.6+ KB\n"
     ]
    }
   ],
   "source": [
    "scraped_df = read_data(scraped_data_source)\n",
    "seed_df = read_data(seed_data_source)\n",
    "\n",
    "scraped_df.info()\n",
    "seed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fa97d",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Cleaning \"tags\" column\n",
    "- separate the values in the list and choose the unique tag. \n",
    "- Each tag must be just a value. (i.e list of len 1)\n",
    "    + no # in value \n",
    "    + no duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b49919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = df.drop(columns=[col for col in ['pricing', 'page'] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        logger.info(\"Columns dropped and null values dropped.\",\n",
    "                    extra={\n",
    "                     \"Cols dropped\": ['pricing', 'page'],\n",
    "                     \"Null Values Dropped\": len(df) - len(new_df)\n",
    "                    }\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised {e}! Is the input a dataframe? Use a pandas dataframe.\",  exc_info=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_hashtags(tags):\n",
    "    try:\n",
    "        if isinstance(tags, list):\n",
    "            clean = [tag for tag in tags if '#' not in tag]\n",
    "        elif isinstance(tags, str):\n",
    "            clean = [tags] if \"#\" not in tags else []\n",
    "        else:\n",
    "            clean = []\n",
    "        clean = ','.join(clean)\n",
    "\n",
    "        if len(clean) < 4:\n",
    "            clean = clean.upper()\n",
    "        else:\n",
    "            clean = clean.lower().capitalize()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at tags column cleaning {e}! Use tags column.\",  exc_info=True)\n",
    "    return clean\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    try:\n",
    "        # df = baseline_cleaning(df=scraped_df)\n",
    "        df = df.drop(columns=[col for col in ['pricing', 'page'] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "        if 'tags' in df.columns:\n",
    "            new_df['tags'] = new_df['tags'].apply(remove_hashtags)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        logger.info(\"Columns dropped and null values dropped.\",\n",
    "                    extra={\n",
    "                     \"Cols dropped\": ['pricing', 'page'],\n",
    "                     \"Null Values Dropped\": len(df) - len(new_df)\n",
    "                    }\n",
    "                    )\n",
    "        logger.info(\"Tags Column Successfully cleaned.\")\n",
    "        logger.info(\"Data successfully cleaned!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at full cleaning process: {e}!\",  exc_info=True)\n",
    "    return new_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfcfc1",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef8b05bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4102930063.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif df.\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def needs_transformation(df):\n",
    "    # Add your logic here (e.g., missing columns, data types)\n",
    "    # check for missing values,\n",
    "\n",
    "    if df.\n",
    "\n",
    "    return some_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3606b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_created_at(filepath: str) -> str:\n",
    "    try:\n",
    "        created_timestamp = os.path.getctime(filepath)\n",
    "        created_date = datetime.fromtimestamp(created_timestamp)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised: {e}!\", exc_info=True)\n",
    "    return created_date.strftime(\"%Y-%M-%d\")\n",
    "\n",
    "\n",
    "def transform_data(df: pd.DataFrame, source = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        created_day = get_created_at(scraped_data_source)\n",
    "        if 'source' in df.columns:\n",
    "            if df['source'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['source'] = source\n",
    "        else:\n",
    "            df['source'] = source\n",
    "\n",
    "\n",
    "        if 'created_at' in df.columns:\n",
    "            if df['created_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['created_at'] = created_day\n",
    "        else:\n",
    "            df['created_at'] = created_day\n",
    "        if 'updated_at' in df.columns:\n",
    "            if df['updated_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['updated_at'] = None\n",
    "        else:\n",
    "            df['updated_at'] = None\n",
    "        \n",
    "\n",
    "        if 'trending' not in df.columns:\n",
    "            df['trending'] = None\n",
    "        else:\n",
    "            df['trending'] = df['trending'].replace({\"Low\": False, \"Medium\": True, \"High\": True})\n",
    "    \n",
    "        trans_df = df.rename(columns={'url': 'homepage_url', 'tags': 'category'})\n",
    "        \n",
    "        trans_df['created_at'] = pd.to_datetime(trans_df['created_at'], format=\"%Y-%M-%d\", errors=\"coerce\")\n",
    "        trans_df['updated_at'] = pd.to_datetime(trans_df['updated_at'], format=\"%Y-%M-%d\", errors=\"coerce\")\n",
    "        trans_df['trending'] = trans_df['trending'].notna().astype(bool)\n",
    "\n",
    "        logger.info(\"Data successfully transformed!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at transformation: {e}!\", exc_info=True)\n",
    "    return trans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f82458",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bca5e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_et(source: str) -> pd.DataFrame:\n",
    "    # Extract\n",
    "    scraped_df = read_data(source)\n",
    "\n",
    "    # Clean\n",
    "    clean_df = clean_data(scraped_df)\n",
    "\n",
    "    # Transform\n",
    "    trans_df = transform_data(clean_df, source='https://aitoolsdirectory.com/')\n",
    "\n",
    "    # Load\n",
    "    \"I dey come. Mapami\"\n",
    "\n",
    "    return trans_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e022e0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 14:47:04,328 - INFO - Columns dropped and null values dropped.\n",
      "2025-05-28 14:47:04,331 - INFO - Tags Column Successfully cleaned.\n",
      "2025-05-28 14:47:04,333 - INFO - Data successfully cleaned!\n",
      "C:\\Users\\APIN-PC\\AppData\\Local\\Temp\\ipykernel_24432\\1055242074.py:41: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['trending'] = df['trending'].replace({\"Low\": False, \"Medium\": True, \"High\": True})\n",
      "2025-05-28 14:47:04,447 - INFO - Data successfully transformed!\n",
      "2025-05-28 14:47:04,598 - INFO - Columns dropped and null values dropped.\n",
      "2025-05-28 14:47:04,606 - INFO - Tags Column Successfully cleaned.\n",
      "2025-05-28 14:47:04,611 - INFO - Data successfully cleaned!\n",
      "2025-05-28 14:47:04,749 - INFO - Data successfully transformed!\n"
     ]
    }
   ],
   "source": [
    "seed_clean_df = run_basic_et(source=seed_data_source)\n",
    "scraped_clean_df = run_basic_et(source=scraped_data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e790f781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>homepage_url</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HubSpot Marketing AI</td>\n",
       "      <td>AI-powered marketing automation and content ge...</td>\n",
       "      <td>https://www.hubspot.com/products/marketing/art...</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>HubSpot</td>\n",
       "      <td>2023-01-15 00:01:00</td>\n",
       "      <td>2024-01-01 00:10:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jasper.ai</td>\n",
       "      <td>AI content creation platform for marketing cop...</td>\n",
       "      <td>https://www.jasper.ai/</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Jasper</td>\n",
       "      <td>2021-01-01 00:02:00</td>\n",
       "      <td>2024-01-15 00:09:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Copy.ai</td>\n",
       "      <td>AI-powered copywriting assistant for marketing...</td>\n",
       "      <td>https://www.copy.ai/</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Copy.ai</td>\n",
       "      <td>2020-01-01 00:10:00</td>\n",
       "      <td>2024-01-10 00:10:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MarketMuse</td>\n",
       "      <td>AI content planning and optimization for SEO a...</td>\n",
       "      <td>https://www.marketmuse.com/</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>MarketMuse</td>\n",
       "      <td>2018-01-01 00:05:00</td>\n",
       "      <td>2024-01-20 00:08:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Persado</td>\n",
       "      <td>AI language generation for marketing messaging...</td>\n",
       "      <td>https://www.persado.com/</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Persado</td>\n",
       "      <td>2012-01-01 00:03:00</td>\n",
       "      <td>2024-01-30 00:07:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Eleven Labs</td>\n",
       "      <td>AI voice synthesis and cloning platform</td>\n",
       "      <td>https://elevenlabs.io/</td>\n",
       "      <td>Others</td>\n",
       "      <td>Eleven Labs</td>\n",
       "      <td>2022-01-01 00:01:00</td>\n",
       "      <td>2024-01-20 00:08:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Murf AI</td>\n",
       "      <td>AI voice generation for content creation</td>\n",
       "      <td>https://murf.ai/</td>\n",
       "      <td>Others</td>\n",
       "      <td>Murf</td>\n",
       "      <td>2020-01-01 00:01:00</td>\n",
       "      <td>2024-01-30 00:07:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Synthesia</td>\n",
       "      <td>AI video creation with synthetic avatars</td>\n",
       "      <td>https://www.synthesia.io/</td>\n",
       "      <td>Others</td>\n",
       "      <td>Synthesia</td>\n",
       "      <td>2017-01-01 00:01:00</td>\n",
       "      <td>2024-01-15 00:08:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>DeepL</td>\n",
       "      <td>AI translation and language processing</td>\n",
       "      <td>https://www.deepl.com/</td>\n",
       "      <td>Others</td>\n",
       "      <td>DeepL</td>\n",
       "      <td>2017-01-01 00:01:00</td>\n",
       "      <td>2024-01-05 00:09:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>GitHub Copilot</td>\n",
       "      <td>AI code completion and programming assistant</td>\n",
       "      <td>https://github.com/features/copilot</td>\n",
       "      <td>Others</td>\n",
       "      <td>GitHub</td>\n",
       "      <td>2021-01-01 00:06:00</td>\n",
       "      <td>2024-01-25 00:09:00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                        description  \\\n",
       "0   HubSpot Marketing AI  AI-powered marketing automation and content ge...   \n",
       "1              Jasper.ai  AI content creation platform for marketing cop...   \n",
       "2                Copy.ai  AI-powered copywriting assistant for marketing...   \n",
       "3             MarketMuse  AI content planning and optimization for SEO a...   \n",
       "4                Persado  AI language generation for marketing messaging...   \n",
       "..                   ...                                                ...   \n",
       "67           Eleven Labs            AI voice synthesis and cloning platform   \n",
       "68               Murf AI           AI voice generation for content creation   \n",
       "69             Synthesia           AI video creation with synthetic avatars   \n",
       "70                 DeepL             AI translation and language processing   \n",
       "71        GitHub Copilot       AI code completion and programming assistant   \n",
       "\n",
       "                                         homepage_url   category       source  \\\n",
       "0   https://www.hubspot.com/products/marketing/art...  Marketing      HubSpot   \n",
       "1                              https://www.jasper.ai/  Marketing       Jasper   \n",
       "2                                https://www.copy.ai/  Marketing      Copy.ai   \n",
       "3                         https://www.marketmuse.com/  Marketing   MarketMuse   \n",
       "4                            https://www.persado.com/  Marketing      Persado   \n",
       "..                                                ...        ...          ...   \n",
       "67                             https://elevenlabs.io/     Others  Eleven Labs   \n",
       "68                                   https://murf.ai/     Others         Murf   \n",
       "69                          https://www.synthesia.io/     Others    Synthesia   \n",
       "70                             https://www.deepl.com/     Others        DeepL   \n",
       "71                https://github.com/features/copilot     Others       GitHub   \n",
       "\n",
       "            created_at          updated_at  trending  \n",
       "0  2023-01-15 00:01:00 2024-01-01 00:10:00      True  \n",
       "1  2021-01-01 00:02:00 2024-01-15 00:09:00      True  \n",
       "2  2020-01-01 00:10:00 2024-01-10 00:10:00      True  \n",
       "3  2018-01-01 00:05:00 2024-01-20 00:08:00      True  \n",
       "4  2012-01-01 00:03:00 2024-01-30 00:07:00      True  \n",
       "..                 ...                 ...       ...  \n",
       "67 2022-01-01 00:01:00 2024-01-20 00:08:00      True  \n",
       "68 2020-01-01 00:01:00 2024-01-30 00:07:00      True  \n",
       "69 2017-01-01 00:01:00 2024-01-15 00:08:00      True  \n",
       "70 2017-01-01 00:01:00 2024-01-05 00:09:00      True  \n",
       "71 2021-01-01 00:06:00 2024-01-25 00:09:00      True  \n",
       "\n",
       "[72 rows x 8 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890b3c8",
   "metadata": {},
   "source": [
    "## Merging Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.merge(seed_clean_df, scraped_clean_df, how='outer')\n",
    "comp_df.drop_duplicates(subset='name', inplace=True)\n",
    "comp_df = comp_df.reset_index(drop=True)\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d30b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_list = pd.Series(comp_df['name'])\n",
    "ai_list.to_csv('../etl/data/Ai_tools_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d4b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                    object\n",
       "description             object\n",
       "homepage_url            object\n",
       "category                object\n",
       "source                  object\n",
       "created_at      datetime64[ns]\n",
       "updated_at      datetime64[ns]\n",
       "trending                  bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_clean_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8d847e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 13:12:17,675 - INFO - Columns with missing values present updated_at\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "description\n",
      "homepage_url\n",
      "category\n",
      "source\n"
     ]
    }
   ],
   "source": [
    "scraped_df.isna().sum()\n",
    "\n",
    "for col in scraped_clean_df.columns:\n",
    "    if scraped_clean_df[col].isna().sum() > 0:\n",
    "        logger.info(\"Columns with missing values present %s\", col)\n",
    "    if scraped_clean_df[col].dtype == list:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3718cf",
   "metadata": {},
   "source": [
    "## Functions to Work On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f9466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name           object\n",
       "description    object\n",
       "url            object\n",
       "tags           object\n",
       "pricing        object\n",
       "page            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def delta_check(new_df, existing_df):\n",
    "    # Merge and check for differences\n",
    "    merged = new_df.merge(\n",
    "        existing_df, on=[\"name\", \"homepage_url\"], how=\"left\", suffixes=(\"\", \"_existing\")\n",
    "    )\n",
    "    changed = merged[\n",
    "        (merged[\"email\"] != merged[\"email_existing\"])\n",
    "        | (merged[\"phone\"] != merged[\"phone_existing\"])\n",
    "    ]\n",
    "    return changed[new_df.columns] \n",
    "\n",
    "\n",
    "def fetch_existing_records(conn):\n",
    "    return pd.read_sql(\"SELECT name, homepage_url, email, phone FROM agents\", conn)\n",
    "\n",
    "\n",
    "\n",
    "def upsert_records(conn, df):\n",
    "    cursor = conn.cursor()\n",
    "    for _, row in df.iterrows():\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO agents (name, homepage_url, email, phone)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "            ON CONFLICT(name, homepage_url)\n",
    "            DO UPDATE SET email=excluded.email, phone=excluded.phone\n",
    "        \"\"\",\n",
    "            (row[\"name\"], row[\"homepage_url\"], row[\"email\"], row[\"phone\"]),\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def etl_job(source_path):\n",
    "    df = read_data(source_path)\n",
    "\n",
    "    if needs_transformation(df):\n",
    "        df = transform_data(df)\n",
    "\n",
    "    with engine.connect(\"agents.db\") as conn:\n",
    "        existing_df = fetch_existing_records(conn)\n",
    "        delta_df = delta_check(df, existing_df)\n",
    "\n",
    "        if not delta_df.empty:\n",
    "            upsert_records(conn, delta_df)\n",
    "            print(f\"Upserted {len(delta_df)} records.\")\n",
    "        else:\n",
    "            print(\"No changes detected. Idempotent run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607bc10d",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Pending tasks\n",
    "- Load data into db\n",
    "- Get created_at for scraped ai tools. \n",
    "\n",
    "### Action\n",
    "- Create a new csv file to combine both seeded and scraped tools.  `done`\n",
    "- Check for duplicates. `done`\n",
    "- Log duplicates and update duplicates `pending`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95e82cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'description', 'homepage_url', 'category', 'source',\n",
       "       'created_at', 'updated_at', 'trending'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-directory-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
