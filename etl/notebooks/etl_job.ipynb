{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675d500a",
   "metadata": {},
   "source": [
    "## TASKS\n",
    "```\n",
    "DESCRIPTION                             STATUS \n",
    "- Load seed data into DB.               done\n",
    "- Clean and transform scraped data      done\n",
    "- Create an idempotent etl job          ongoing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5008c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c588289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8b61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\20250601_213652_ai_tools_scraped.csv\"\n",
    "seed_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\seeded_ai_agents.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a81ac",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30430eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(source_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Data Path\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises error for unsupported data type.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas Dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ext = Path(source_path).suffix\n",
    "        if ext == \".csv\":\n",
    "            return pd.read_csv(source_path)\n",
    "        elif ext == \".json\":\n",
    "            return pd.read_json(source_path)\n",
    "        elif ext == \".parquet\":\n",
    "            return pd.read_parquet(source_path)\n",
    "        logger.info(\"Data successfully read!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}. Unsupported file format! Use csv, json or parquet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a875aff",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69a1ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 12:34:18,982 - ERROR - Error: [Errno 2] No such file or directory: 'C:\\\\Users\\\\APIN PC\\\\OneDrive\\\\Documents\\\\DS\\\\DE_Inter\\\\data_epic_capstone\\\\etl\\\\data\\\\20250601_213652_ai_tools_scraped.csv'. Unsupported file format! Use csv, json or parquet.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m scraped_df = read_data(scraped_data_source)\n\u001b[32m      2\u001b[39m seed_df = read_data(seed_data_source)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mscraped_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m()\n\u001b[32m      5\u001b[39m seed_df.info()\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "scraped_df = read_data(scraped_data_source)\n",
    "seed_df = read_data(seed_data_source)\n",
    "\n",
    "scraped_df.info()\n",
    "seed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fa97d",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Cleaning \"tags\" column\n",
    "- separate the values in the list and choose the unique tag. \n",
    "- Each tag must be just a value. (i.e list of len 1)\n",
    "    + no # in value \n",
    "    + no duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b49919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = df.drop(columns=[col for col in ['pricing', 'page'] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        logger.info(\"Columns dropped and null values dropped.\",\n",
    "                    extra={\n",
    "                     \"Cols dropped\": ['pricing', 'page'],\n",
    "                     \"Null Values Dropped\": len(df) - len(new_df)\n",
    "                    }\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised {e}! Is the input a dataframe? Use a pandas dataframe.\",  exc_info=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_hashtags(tags):\n",
    "    try:\n",
    "        if isinstance(tags, list):\n",
    "            clean = [tag for tag in tags if '#' not in tag]\n",
    "        elif isinstance(tags, str):\n",
    "            clean = [tags] if \"#\" not in tags else []\n",
    "        else:\n",
    "            clean = []\n",
    "        clean = ','.join(clean)\n",
    "\n",
    "        if len(clean) < 4:\n",
    "            clean = clean.upper()\n",
    "        else:\n",
    "            clean = clean.lower().capitalize()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at tags column cleaning {e}! Use tags column.\",  exc_info=True)\n",
    "    return clean\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    try:\n",
    "        # df = baseline_cleaning(df=scraped_df)\n",
    "        df = df.drop(columns=[col for col in ['pricing', 'page'] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "        if 'tags' in df.columns:\n",
    "            new_df['tags'] = new_df['tags'].apply(remove_hashtags)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        logger.info(\"Columns dropped and null values dropped.\",\n",
    "                    extra={\n",
    "                     \"Cols dropped\": ['pricing', 'page'],\n",
    "                     \"Null Values Dropped\": len(df) - len(new_df)\n",
    "                    }\n",
    "                    )\n",
    "        logger.info(\"Tags Column Successfully cleaned.\")\n",
    "        logger.info(\"Data successfully cleaned!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at full cleaning process: {e}!\",  exc_info=True)\n",
    "    return new_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfcfc1",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_created_at(filepath: str) -> str:\n",
    "    try:\n",
    "        created_timestamp = os.path.getctime(filepath)\n",
    "        created_date = datetime.fromtimestamp(created_timestamp)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised: {e}!\", exc_info=True)\n",
    "    return created_date.strftime(\"%Y-%M-%d\")\n",
    "\n",
    "\n",
    "def transform_data(df: pd.DataFrame, source = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        created_day = get_created_at(scraped_data_source)\n",
    "        if 'source' in df.columns:\n",
    "            if df['source'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['source'] = source\n",
    "        else:\n",
    "            df['source'] = source\n",
    "\n",
    "\n",
    "        if 'created_at' in df.columns:\n",
    "            if df['created_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['created_at'] = created_day\n",
    "        else:\n",
    "            df['created_at'] = created_day\n",
    "        if 'updated_at' in df.columns:\n",
    "            if df['updated_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['updated_at'] = None\n",
    "        else:\n",
    "            df['updated_at'] = None\n",
    "        \n",
    "\n",
    "        if 'trending' not in df.columns:\n",
    "            df['trending'] = None\n",
    "            df['trending'] = df['trending'].notna().astype(bool)\n",
    "        else:\n",
    "            df[\"trending\"] = df[\"trending\"].apply(\n",
    "                lambda x: False if x == 'Low' else True\n",
    "                )\n",
    "            df['trending'] = df['trending'].astype(bool)\n",
    "    \n",
    "        trans_df = df.rename(columns={'url': 'homepage_url', 'tags': 'category'})\n",
    "        \n",
    "        trans_df['created_at'] = pd.to_datetime(trans_df['created_at'], format=\"%Y-%M-%d\", errors=\"coerce\")\n",
    "        trans_df['updated_at'] = pd.to_datetime(trans_df['updated_at'], format=\"%Y-%M-%d\", errors=\"coerce\")\n",
    "\n",
    "        logger.info(\"Data successfully transformed!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at transformation: {e}!\", exc_info=True)\n",
    "    return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba06c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_dfs(new_df, existing_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merging DFs to extract unique ai_tools\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DF with unique Ai tools\n",
    "    \"\"\"\n",
    "    try:\n",
    "        merged_df = pd.merge(existing_df, new_df, how=\"outer\", suffixes=\"_existing\")\n",
    "        merged_df.drop_duplicates(subset=[\n",
    "            \"name\", \"homepage_url\"\n",
    "            ], inplace=True)\n",
    "        merged_df = merged_df.reset_index(drop=True)\n",
    "        logger.info(\"Existing DB Data and Scraped Data successfully merged!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error merging DFs: %s\", e, exc_info=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f82458",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_etl() -> pd.DataFrame:\n",
    "    # Extract\n",
    "    scraped_df = read_data(scraped_data_source)\n",
    "\n",
    "    # Clean\n",
    "    clean_scraped_df = clean_data(scraped_df)\n",
    "\n",
    "    # Transform\n",
    "    trans_scraped_df = transform_data(\n",
    "        clean_scraped_df, source=\"https://aitoolsdirectory.com/\"\n",
    "    )\n",
    "\n",
    "    return trans_scraped_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a3101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 22:03:55,423 - INFO - Columns dropped and null values dropped.\n",
      "2025-06-01 22:03:55,431 - INFO - Tags Column Successfully cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 22:03:55,471 - INFO - Data successfully cleaned!\n",
      "2025-06-01 22:03:55,637 - INFO - Data successfully transformed!\n"
     ]
    }
   ],
   "source": [
    "comp_df = run_basic_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c39c5f",
   "metadata": {},
   "source": [
    "## Idempotent ETL Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75079cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ai_tools ETL Local DB Setup and data upload\n",
    "\n",
    "Name: Arowosegbe Victor Iyanuoluwa\\n\n",
    "Email: Iyanuvicky@gmail.com\\n\n",
    "GitHub: https://github.com/Iyanuvicky22/projects\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    Column,\n",
    "    String,\n",
    "    Text,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    "    func,\n",
    "    Integer,\n",
    ")\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"\n",
    "    Database connector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(DB_URL)\n",
    "        Session = sessionmaker(bind=engine, autoflush=False)\n",
    "        Base.metadata.create_all(engine)\n",
    "        logger.info(\"Database succesfully connected to.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(\"Databse connection error: %s\", e, exc_info=True)\n",
    "    return Session, engine\n",
    "\n",
    "\n",
    "class Agent(Base):\n",
    "    \"\"\"\n",
    "    Agents table model creation\n",
    "    Args:\n",
    "        Base (): SQLAlchemy Base model\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = \"agents\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    name = Column(String, nullable=False, index=True)\n",
    "    description = Column(Text)\n",
    "    homepage_url = Column(String)\n",
    "    category = Column(String)\n",
    "    source = Column(String)\n",
    "    trending = Column(Boolean, default=False)\n",
    "    created_at = Column(DateTime, server_default=func.now(), nullable=False)\n",
    "    updated_at = Column(\n",
    "        DateTime, server_default=func.now(), onupdate=func.now(), nullable=False\n",
    "    )\n",
    "\n",
    "\n",
    "def load_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to load data into the Database (PostgreSQL).\n",
    "    Args:\n",
    "        df (pd.DataFrame): Cleaned and Transformed data to be loaded.\n",
    "    \"\"\"\n",
    "    Session, engine = connect_db()\n",
    "    data = df\n",
    "\n",
    "    def change_to_none(row):\n",
    "        row.replace(pd.NaT, '')\n",
    "        return row\n",
    "\n",
    "\n",
    "    with Session.begin() as session:\n",
    "        for _, row in data.iterrows():\n",
    "            ai_tool = session.query(Agent).filter_by(name=str(row[\"name\"])).first()\n",
    "\n",
    "            if not ai_tool:\n",
    "                agent = Agent(\n",
    "                    name=str(row[\"name\"]),\n",
    "                    description=str(row[\"description\"]),\n",
    "                    homepage_url=row[\"homepage_url\"],\n",
    "                    category=row[\"category\"],\n",
    "                    source=row[\"source\"],\n",
    "                    trending=row[\"trending\"],\n",
    "                    created_at=row[\"created_at\"],\n",
    "                    updated_at=row[\"updated_at\"],\n",
    "                )\n",
    "                session.add(agent)\n",
    "        session.commit()\n",
    "        logger.info(\"Data successfully loaded in database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f49dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\20250601_213652_ai_tools_scraped.csv\"\n",
    "\n",
    "def trans_load_seed_df():\n",
    "    \"\"\"\n",
    "    Function to Load Seed Data into Database\n",
    "    \"\"\"\n",
    "    data = read_data(source_path=path)\n",
    "\n",
    "    clean_df = clean_data(data)\n",
    "\n",
    "    trans_seed_df = transform_data(df=clean_df)\n",
    "\n",
    "    # load_data(trans_seed_df)\n",
    "\n",
    "    return trans_seed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc723e",
   "metadata": {},
   "source": [
    "## Checking ETL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadfc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 22:04:00,715 - INFO - Columns dropped and null values dropped.\n",
      "2025-06-01 22:04:00,733 - INFO - Tags Column Successfully cleaned.\n",
      "2025-06-01 22:04:00,748 - INFO - Data successfully cleaned!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 22:04:00,838 - INFO - Data successfully transformed!\n"
     ]
    }
   ],
   "source": [
    "ab = trans_load_seed_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f992d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4230 entries, 0 to 4229\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   name          4230 non-null   object        \n",
      " 1   description   4230 non-null   object        \n",
      " 2   homepage_url  4230 non-null   object        \n",
      " 3   source        4230 non-null   object        \n",
      " 4   category      4230 non-null   object        \n",
      " 5   created_at    4230 non-null   datetime64[ns]\n",
      " 6   updated_at    0 non-null      datetime64[ns]\n",
      " 7   trending      4230 non-null   bool          \n",
      "dtypes: bool(1), datetime64[ns](2), object(5)\n",
      "memory usage: 235.6+ KB\n"
     ]
    }
   ],
   "source": [
    "ab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457eeb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>homepage_url</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X-Design</td>\n",
       "      <td>Specialist AI tool for product photography. Up...</td>\n",
       "      <td>https://www.x-design.com/</td>\n",
       "      <td>Image Editing</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KIVA</td>\n",
       "      <td>KIVA is an AI SEO tool that automates keyword ...</td>\n",
       "      <td>https://wellows.com/kiva/</td>\n",
       "      <td>SEO</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parliant</td>\n",
       "      <td>Parliant AI offers AI-driven conversational su...</td>\n",
       "      <td>https://www.parliant.ai/</td>\n",
       "      <td>Business Intelligence</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Galaxy AI</td>\n",
       "      <td>Your go-to hub for AI tools. This all-in-one A...</td>\n",
       "      <td>https://link.aitoolsdirectory.com/galaxy</td>\n",
       "      <td>Productivity</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dropmagic</td>\n",
       "      <td>Build Shopify stores from AliExpress or produc...</td>\n",
       "      <td>https://link.aitoolsdirectory.com/dropmagic</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8066</th>\n",
       "      <td>Photofeeler</td>\n",
       "      <td>Photofeeler provides unbiased photo feedback t...</td>\n",
       "      <td>https://www.photofeeler.com?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8067</th>\n",
       "      <td>Thundr.tv</td>\n",
       "      <td>Anonymous, AI-moderated video and text chat pl...</td>\n",
       "      <td>https://www.thundr.tv?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8068</th>\n",
       "      <td>ShowZone</td>\n",
       "      <td>MLB The Show tools and resources for strategic...</td>\n",
       "      <td>https://showzone.gg?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8069</th>\n",
       "      <td>Dover</td>\n",
       "      <td>Dover helps startups hire top talent with frac...</td>\n",
       "      <td>https://www.dover.com/trial?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8070</th>\n",
       "      <td>DuckDuckGo</td>\n",
       "      <td>Privacy-focused search engine and browser with...</td>\n",
       "      <td>https://duck.ai?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6895 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                                        description  \\\n",
       "0        X-Design  Specialist AI tool for product photography. Up...   \n",
       "1            KIVA  KIVA is an AI SEO tool that automates keyword ...   \n",
       "2        Parliant  Parliant AI offers AI-driven conversational su...   \n",
       "3       Galaxy AI  Your go-to hub for AI tools. This all-in-one A...   \n",
       "4       Dropmagic  Build Shopify stores from AliExpress or produc...   \n",
       "...           ...                                                ...   \n",
       "8066  Photofeeler  Photofeeler provides unbiased photo feedback t...   \n",
       "8067    Thundr.tv  Anonymous, AI-moderated video and text chat pl...   \n",
       "8068     ShowZone  MLB The Show tools and resources for strategic...   \n",
       "8069        Dover  Dover helps startups hire top talent with frac...   \n",
       "8070   DuckDuckGo  Privacy-focused search engine and browser with...   \n",
       "\n",
       "                                        homepage_url                category  \\\n",
       "0                          https://www.x-design.com/           Image Editing   \n",
       "1                          https://wellows.com/kiva/                     SEO   \n",
       "2                           https://www.parliant.ai/   Business Intelligence   \n",
       "3           https://link.aitoolsdirectory.com/galaxy            Productivity   \n",
       "4        https://link.aitoolsdirectory.com/dropmagic               Marketing   \n",
       "...                                              ...                     ...   \n",
       "8066  https://www.photofeeler.com?utm_source=toolify  https://www.toolify.ai   \n",
       "8067        https://www.thundr.tv?utm_source=toolify  https://www.toolify.ai   \n",
       "8068          https://showzone.gg?utm_source=toolify  https://www.toolify.ai   \n",
       "8069  https://www.dover.com/trial?utm_source=toolify  https://www.toolify.ai   \n",
       "8070              https://duck.ai?utm_source=toolify  https://www.toolify.ai   \n",
       "\n",
       "                            source          created_at updated_at  trending  \n",
       "0     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "1     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "2     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "3     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "4     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "...                            ...                 ...        ...       ...  \n",
       "8066                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "8067                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "8068                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "8069                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "8070                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "\n",
       "[6895 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.drop_duplicates(subset=['homepage_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9489ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaT\n"
     ]
    }
   ],
   "source": [
    "ab.duplicated(subset=['name', 'homepage_url']).value_counts()\n",
    "\n",
    "ab[ab.duplicated(subset=['name'])]\n",
    "\n",
    "def turn_to_none(col):\n",
    "    for row in col:\n",
    "        if row == pd.NaT:\n",
    "            row = None\n",
    "        else: \n",
    "            row = row\n",
    "    return row\n",
    "\n",
    "ac = turn_to_none(ab['updated_at'])\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d23295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 21:32:23,276 - ERROR - Databse connection error: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 145, in __init__\n",
      "    self._dbapi_connection = engine.raw_connection()\n",
      "                             ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3297, in raw_connection\n",
      "    return self.pool.connect()\n",
      "           ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 449, in connect\n",
      "    return _ConnectionFairy._checkout(self)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 1264, in _checkout\n",
      "    fairy = _ConnectionRecord.checkout(pool)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 713, in checkout\n",
      "    rec = pool._do_get()\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py\", line 179, in _do_get\n",
      "    with util.safe_reraise():\n",
      "         ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 224, in __exit__\n",
      "    raise exc_value.with_traceback(exc_tb)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py\", line 177, in _do_get\n",
      "    return self._create_connection()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 390, in _create_connection\n",
      "    return _ConnectionRecord(self)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 675, in __init__\n",
      "    self.__connect()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 901, in __connect\n",
      "    with util.safe_reraise():\n",
      "         ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 224, in __exit__\n",
      "    raise exc_value.with_traceback(exc_tb)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 897, in __connect\n",
      "    self.dbapi_connection = connection = pool._invoke_creator(self)\n",
      "                                         ~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py\", line 646, in connect\n",
      "    return dialect.connect(*cargs, **cparams)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py\", line 625, in connect\n",
      "    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py\", line 135, in connect\n",
      "    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n",
      "psycopg2.OperationalError: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\APIN-PC\\AppData\\Local\\Temp\\ipykernel_6896\\203178352.py\", line 45, in connect_db\n",
      "    Base.metadata.create_all(engine)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py\", line 5924, in create_all\n",
      "    bind._run_ddl_visitor(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~^\n",
      "        ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3247, in _run_ddl_visitor\n",
      "    with self.begin() as conn:\n",
      "         ~~~~~~~~~~^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 141, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3237, in begin\n",
      "    with self.connect() as conn:\n",
      "         ~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3273, in connect\n",
      "    return self._connection_cls(self)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 147, in __init__\n",
      "    Connection._handle_dbapi_exception_noconnection(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        err, dialect, engine\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 2436, in _handle_dbapi_exception_noconnection\n",
      "    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 145, in __init__\n",
      "    self._dbapi_connection = engine.raw_connection()\n",
      "                             ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3297, in raw_connection\n",
      "    return self.pool.connect()\n",
      "           ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 449, in connect\n",
      "    return _ConnectionFairy._checkout(self)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 1264, in _checkout\n",
      "    fairy = _ConnectionRecord.checkout(pool)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 713, in checkout\n",
      "    rec = pool._do_get()\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py\", line 179, in _do_get\n",
      "    with util.safe_reraise():\n",
      "         ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 224, in __exit__\n",
      "    raise exc_value.with_traceback(exc_tb)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py\", line 177, in _do_get\n",
      "    return self._create_connection()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 390, in _create_connection\n",
      "    return _ConnectionRecord(self)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 675, in __init__\n",
      "    self.__connect()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 901, in __connect\n",
      "    with util.safe_reraise():\n",
      "         ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 224, in __exit__\n",
      "    raise exc_value.with_traceback(exc_tb)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 897, in __connect\n",
      "    self.dbapi_connection = connection = pool._invoke_creator(self)\n",
      "                                         ~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py\", line 646, in connect\n",
      "    return dialect.connect(*cargs, **cparams)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py\", line 625, in connect\n",
      "    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py\", line 135, in connect\n",
      "    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n",
      "sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3297\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3276\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3277\u001b[39m \n\u001b[32m   3278\u001b[39m \u001b[33;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3295\u001b[39m \n\u001b[32m   3296\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    443\u001b[39m \n\u001b[32m    444\u001b[39m \u001b[33;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \n\u001b[32m    448\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:713\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:675\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:901\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:897\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    896\u001b[39m \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    644\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:625\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    624\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m         conn.commit()\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m db_df\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m db_df = \u001b[43mfetch_db_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mfetch_db_records\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_db_records\u001b[39m():\n\u001b[32m      4\u001b[39m     session, engine = connect_db()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m      7\u001b[39m         db_df = pd.read_sql(\u001b[33m\"\u001b[39m\u001b[33mSELECT * from agents\u001b[39m\u001b[33m\"\u001b[39m, con=conn)\n\u001b[32m      8\u001b[39m         conn.commit()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3273\u001b[39m, in \u001b[36mEngine.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3250\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Connection:\n\u001b[32m   3251\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[32m   3252\u001b[39m \n\u001b[32m   3253\u001b[39m \u001b[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3270\u001b[39m \n\u001b[32m   3271\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:147\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = engine.raw_connection()\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         \u001b[43mConnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2436\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception_noconnection\u001b[39m\u001b[34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[39m\n\u001b[32m   2434\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2435\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2437\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2438\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    147\u001b[39m         Connection._handle_dbapi_exception_noconnection(\n\u001b[32m    148\u001b[39m             err, dialect, engine\n\u001b[32m    149\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3297\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m   3276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3277\u001b[39m \n\u001b[32m   3278\u001b[39m \u001b[33;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3295\u001b[39m \n\u001b[32m   3296\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    443\u001b[39m \n\u001b[32m    444\u001b[39m \u001b[33;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \n\u001b[32m    448\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1256\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_checkout\u001b[39m(\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1261\u001b[39m     fairy: Optional[_ConnectionFairy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1262\u001b[39m ) -> _ConnectionFairy:\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m         fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1267\u001b[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:713\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    711\u001b[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    716\u001b[39m     dbapi_connection = rec.get_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_connection()\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inc_overflow():\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m util.safe_reraise():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ConnectionPoolEntry:\n\u001b[32m    388\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:675\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28mself\u001b[39m.__pool = pool\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:901\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:897\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m     pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    643\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    644\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:625\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    624\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    134\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    137\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "def fetch_db_records():\n",
    "    session, engine = connect_db()\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        db_df = pd.read_sql(\"SELECT * from agents\", con=conn)\n",
    "        conn.commit()\n",
    "    return db_df\n",
    "\n",
    "db_df = fetch_db_records()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4822601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trending\n",
       "True     60\n",
       "False    12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_df.trending.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d97d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc3718cf",
   "metadata": {},
   "source": [
    "## Functions to Work On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_check(new_df, existing_df):\n",
    "    # Merge and check for differences\n",
    "    merged = new_df.merge(\n",
    "        existing_df, on=[\"name\", \"homepage_url\"], how=\"left\", suffixes=(\"\", \"_existing\")\n",
    "    )\n",
    "    changed = merged[\n",
    "        (merged[\"name\"] != merged[\"name_existing\"])\n",
    "        | (merged[\"homepage_url\"] != merged[\"homepage_url_existing\"])\n",
    "    ]\n",
    "    return changed[new_df.columns] \n",
    "\n",
    "\n",
    "\n",
    "def upsert_records(conn, df):\n",
    "    cursor = conn.cursor()\n",
    "    for _, row in df.iterrows():\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO agents (name, homepage_url, email, phone)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "            ON CONFLICT(name, homepage_url)\n",
    "            DO UPDATE SET email=excluded.email, phone=excluded.phone\n",
    "        \"\"\",\n",
    "            (row[\"name\"], row[\"homepage_url\"], row[\"email\"], row[\"phone\"]),\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def etl_job(source_path):\n",
    "    df = read_data(source_path)\n",
    "\n",
    "    if needs_transformation(df):\n",
    "        df = transform_data(df)\n",
    "\n",
    "    with engine.connect(\"agents.db\") as conn:\n",
    "        existing_df = fetch_existing_records(conn)\n",
    "        delta_df = delta_check(df, existing_df)\n",
    "\n",
    "        if not delta_df.empty:\n",
    "            upsert_records(conn, delta_df)\n",
    "            print(f\"Upserted {len(delta_df)} records.\")\n",
    "        else:\n",
    "            print(\"No changes detected. Idempotent run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607bc10d",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Pending tasks\n",
    "- Load Seed data separately.    ```done```\n",
    "- Load scraped data and check for duplicates with name(lower) and homepage_url.\n",
    "- Write tests to check for:\n",
    "    + test for duplicates.\n",
    "    + test for invalid rows.\n",
    "    + test for correct upserts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d54bb",
   "metadata": {},
   "source": [
    "## FUNCTIONALITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b4ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7b129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='.env')\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "s3 = boto3.client(\"s3\", region_name=AWS_REGION,\n",
    "                  aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                  aws_secret_access_key=AWS_SECRET_KEY)\n",
    "\n",
    "bucket_name = 'scraped-ai-agent'\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    filemode='w',\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fa685",
   "metadata": {},
   "source": [
    "### Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a761a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(source_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Data Path\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises error for unsupported data type.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas Dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ext = Path(source_path).suffix\n",
    "        if ext == \".csv\":\n",
    "            return pd.read_csv(source_path)\n",
    "        elif ext == \".json\":\n",
    "            return pd.read_json(source_path)\n",
    "        elif ext == \".parquet\":\n",
    "            return pd.read_parquet(source_path)\n",
    "        logger.info(\"Data successfully read!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"Error raised at data loading. Unsupported file format! Use csv, json or parquet: %s\",\n",
    "            e,\n",
    "            exc_info=True,\n",
    "        )\n",
    "\n",
    "\n",
    "def remove_hashtags(tags):\n",
    "    \"\"\"\n",
    "    Method to clean category column from \"https://aitoolsdirectory.com/\"\n",
    "    Args:\n",
    "        tags (Series): Column to be cleaned\n",
    "\n",
    "    Returns:\n",
    "        Series: Cleaned column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(tags, list):\n",
    "            clean = [tag for tag in tags if \"#\" not in tag]\n",
    "        elif isinstance(tags, str):\n",
    "            clean = [tags] if \"#\" not in tags else []\n",
    "        else:\n",
    "            clean = []\n",
    "        clean = \",\".join(clean)\n",
    "\n",
    "        if len(clean) < 4:\n",
    "            clean = clean.upper()\n",
    "        else:\n",
    "            clean = clean.lower().capitalize()\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error Raised at tags column cleaning:  %s\", e, exc_info=True)\n",
    "    return clean\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Custom function to clean scraped/manual ai_tools dataset\n",
    "    Args:\n",
    "        df (pd.DataFrame): Scraped/manually created ai_tools dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned ai_tools dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.drop(columns=[col for col in [\"pricing\", \"page\"] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "        if \"category\" in df.columns:\n",
    "            new_df[\"category\"] = new_df[\"category\"].apply(remove_hashtags)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        logger.info(\n",
    "            \"Columns dropped and null values dropped.\",\n",
    "            extra={\n",
    "                \"Cols dropped\": [\"pricing\", \"page\"],\n",
    "                \"Null Values Dropped\": len(df) - len(new_df),\n",
    "            },\n",
    "        )\n",
    "        logger.info(\"Tags Column Successfully cleaned.\")\n",
    "        logger.info(\"Data successfully cleaned!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error Raised at full cleaning process: %s\", e, exc_info=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def get_created_at(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracting file creation date\n",
    "    Args:\n",
    "        filepath (str): filepath\n",
    "\n",
    "    Returns:\n",
    "        str: creation time in strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        created_timestamp = os.path.getctime(filepath)\n",
    "        created_date = datetime.fromtimestamp(created_timestamp)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised: {e}!\", exc_info=True)\n",
    "    return created_date.strftime(\"%Y-%M-%d\")\n",
    "\n",
    "\n",
    "def transform_data(df: pd.DataFrame, source=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Custom function to transform any scraped and cleaned ai_tools dataset.\n",
    "    Args:\n",
    "        df (pd.DataFrame): Clean ai_tools dataset\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed ai_tools dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # created_day = get_created_at(scraped_data_source)\n",
    "        if \"source\" in df.columns:\n",
    "            if df[\"source\"] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df[\"source\"] = source\n",
    "        else:\n",
    "            df[\"source\"] = source\n",
    "\n",
    "        if \"created_at\" in df.columns:\n",
    "            if df['created_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df[\"created_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            df[\"created_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        df[\"updated_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        if \"trending\" not in df.columns:\n",
    "            df[\"trending\"] = 0\n",
    "            # df[\"trending\"] = df[\"trending\"].notna().astype(bool)\n",
    "        else:\n",
    "            df[\"trending\"] = df[\"trending\"].apply(\n",
    "                lambda x: 0 if x == \"Low\" else 1\n",
    "            )\n",
    "        df[\"trending\"] = 0\n",
    "\n",
    "        trans_df = df.rename(columns={\"url\": \"homepage_url\", \"tags\": \"category\"})\n",
    "\n",
    "        trans_df[\"created_at\"] = pd.to_datetime(\n",
    "            trans_df[\"created_at\"], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    "        )\n",
    "        trans_df[\"updated_at\"] = pd.to_datetime(\n",
    "            trans_df[\"updated_at\"], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    "        )\n",
    "        trans_df = trans_df.drop_duplicates(subset=['name', 'homepage_url'])\n",
    "\n",
    "        logger.info(\"Data successfully transformed!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error Raised at transformation: %s\", e, exc_info=True)\n",
    "    return trans_df\n",
    "\n",
    "\n",
    "def merging_dfs(new_df, existing_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merging DFs to extract unique ai_tools\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DF with unique Ai tools\n",
    "    \"\"\"\n",
    "    try:\n",
    "        merged_df = pd.merge(new_df, existing_df, how=\"outer\")\n",
    "        merged_df.drop_duplicates(subset=[\n",
    "            \"name\", \"homepage_url\"\n",
    "            ], inplace=True)\n",
    "        merged_df = merged_df.reset_index(drop=True)\n",
    "        logger.info(\"Existing DB Data and Scraped Data successfully merged!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error merging DFs: %s\", e, exc_info=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def fetch_db_records():\n",
    "    session, engine = connect_db()\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        db_df = pd.read_sql(\"SELECT * from agents\", con=conn)\n",
    "        conn.commit()\n",
    "    return db_df\n",
    "\n",
    "\n",
    "def dump_raw_data_to_s3(file_path: str):\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, f\"{os.path.basename(file_path)}\")\n",
    "        logger.info(f\"Successfully upload to s3://{bucket_name}/{file_path}\")\n",
    "        os.remove(file_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Uploading failed: {e}\")\n",
    "\n",
    "\n",
    "def fetch_latest_csv_from_s3(download_dir='downloads'):\n",
    "\n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        contents = response.get('Contents', [])\n",
    "\n",
    "        # Filter for CSV files and sort by last modified time\n",
    "        csv_files = [obj for obj in contents if obj['Key'].endswith('.csv')]\n",
    "        if not csv_files:\n",
    "            logger.info(\"❌ No CSV files found.\")\n",
    "            return None\n",
    "\n",
    "        latest_file = max(csv_files, key=lambda x: x['LastModified'])\n",
    "        latest_key = latest_file['Key']\n",
    "        filename = os.path.basename(latest_key)\n",
    "        local_path = os.path.join(download_dir, filename)\n",
    "\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        s3.download_file(bucket_name, latest_key, local_path)\n",
    "\n",
    "        logger.info(f\"✅ Downloaded latest CSV: {latest_key} → {local_path}\")\n",
    "        return local_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to fetch from S3: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955a812",
   "metadata": {},
   "source": [
    "### Models Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "424b4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    Column,\n",
    "    String,\n",
    "    Text,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    "    func,\n",
    "    Integer,\n",
    ")\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"\n",
    "    Database connector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(DB_URL)\n",
    "        Session = sessionmaker(bind=engine, autoflush=False)\n",
    "        Base.metadata.create_all(engine)\n",
    "        logger.info(\"Database succesfully connected to.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(\"Databse connection error: %s\", e, exc_info=True)\n",
    "    return Session, engine\n",
    "\n",
    "\n",
    "class AiAgent(Base):\n",
    "    \"\"\"\n",
    "    Agents table model creation\n",
    "    Args:\n",
    "        Base (): SQLAlchemy Base model\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = \"ai_agents\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    name = Column(String, nullable=False, index=True)\n",
    "    description = Column(Text)\n",
    "    homepage_url = Column(String)\n",
    "    category = Column(String)\n",
    "    source = Column(String)\n",
    "    trending = Column(Boolean, default=False)\n",
    "    created_at = Column(DateTime, server_default=func.now(), nullable=False)\n",
    "    updated_at = Column(\n",
    "        DateTime, server_default=func.now(), onupdate=func.now(), nullable=False)\n",
    "\n",
    "\n",
    "def load_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to load data into the Database (PostgreSQL).\n",
    "    Args:\n",
    "        df (pd.DataFrame): Cleaned and Transformed data to be loaded.\n",
    "    \"\"\"\n",
    "    Session, engine = connect_db()\n",
    "    data = df\n",
    "\n",
    "    with Session.begin() as session:\n",
    "        try:\n",
    "            for _, row in data.iterrows():\n",
    "                ai_tool = session.query(AiAgent).filter_by(\n",
    "                    name=str(row[\"name\"]),\n",
    "                    homepage_url=row['homepage_url']                                       \n",
    "                    ).first()\n",
    "\n",
    "                if ai_tool:\n",
    "                    ai_tool.description = row.get(\"description\", ai_tool.description)\n",
    "                    ai_tool.category = row.get(\"category\", ai_tool.category)\n",
    "                    ai_tool.source = row.get(\"source\", ai_tool.source)\n",
    "                    ai_tool.updated_at = row.get(\"updated_at\", ai_tool.updated_at)\n",
    "                else:\n",
    "                    ai_tool = AiAgent(\n",
    "                        name=str(row[\"name\"]),\n",
    "                        description=str(row[\"description\"]),\n",
    "                        homepage_url=row[\"homepage_url\"],\n",
    "                        category=row[\"category\"],\n",
    "                        source=row[\"source\"],\n",
    "                        trending=row[\"trending\"],\n",
    "                        created_at=row[\"created_at\"],\n",
    "                        updated_at=row[\"updated_at\"],\n",
    "                    )\n",
    "                    session.add(ai_tool)\n",
    "            session.commit()\n",
    "            logger.info(\"Data successfully loaded in database!\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Data upload failed: %s\", e, exc_info=True)\n",
    "            session.rollback()\n",
    "        finally:\n",
    "            session.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075269e5",
   "metadata": {},
   "source": [
    "### ETL Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0566fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_etl() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Basic ETL Job.\n",
    "    Returns:\n",
    "        pd.DataFrame: Ai tools data to run etl job on.\n",
    "    \"\"\"\n",
    "    # download latest file from s3\n",
    "    scraped_data_source = fetch_latest_csv_from_s3()\n",
    "\n",
    "    scraped_df = read_data(scraped_data_source)\n",
    "\n",
    "    clean_scraped_df = clean_data(scraped_df)\n",
    "\n",
    "    trans_scraped_df = transform_data(clean_scraped_df)\n",
    "    \n",
    "    existing_db_df = fetch_db_records()\n",
    "\n",
    "    final_df = merging_dfs(trans_scraped_df, existing_db_df)\n",
    "\n",
    "    load_data(final_df)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "801221ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 12:38:34,875 - INFO - ✅ Downloaded latest CSV: 20250601_234748_ai_tools_scraped.csv → downloads\\20250601_234748_ai_tools_scraped.csv\n",
      "2025-06-04 12:38:34,965 - INFO - Columns dropped and null values dropped.\n",
      "2025-06-04 12:38:34,966 - INFO - Tags Column Successfully cleaned.\n",
      "2025-06-04 12:38:34,968 - INFO - Data successfully cleaned!\n",
      "2025-06-04 12:38:35,001 - INFO - Data successfully transformed!\n",
      "2025-06-04 12:38:42,784 - INFO - Database succesfully connected to.\n",
      "2025-06-04 12:38:59,049 - INFO - Existing DB Data and Scraped Data successfully merged!\n",
      "2025-06-04 12:39:04,431 - INFO - Database succesfully connected to.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ab = \u001b[43mrun_basic_etl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m ab.info()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mrun_basic_etl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m existing_db_df = fetch_db_records()\n\u001b[32m     18\u001b[39m final_df = merging_dfs(trans_scraped_df, existing_db_df)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data.iterrows():\n\u001b[32m     77\u001b[39m         ai_tool = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAiAgent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_by\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhomepage_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhomepage_url\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m                                       \u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ai_tool:\n\u001b[32m     83\u001b[39m             ai_tool.description = row.get(\u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m, ai_tool.description)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\query.py:2759\u001b[39m, in \u001b[36mQuery.first\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter().first()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2758\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2759\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.first()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\query.py:2857\u001b[39m, in \u001b[36mQuery._iter\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2854\u001b[39m params = \u001b[38;5;28mself\u001b[39m._params\n\u001b[32m   2856\u001b[39m statement = \u001b[38;5;28mself\u001b[39m._statement_20()\n\u001b[32m-> \u001b[39m\u001b[32m2857\u001b[39m result: Union[ScalarResult[_T], Result[_T]] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_sa_orm_load_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2861\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2863\u001b[39m \u001b[38;5;66;03m# legacy: automatically set scalars, unique\u001b[39;00m\n\u001b[32m   2864\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result._attributes.get(\u001b[33m\"\u001b[39m\u001b[33mis_single_entity\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\session.py:2365\u001b[39m, in \u001b[36mSession.execute\u001b[39m\u001b[34m(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event)\u001b[39m\n\u001b[32m   2305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\n\u001b[32m   2306\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2307\u001b[39m     statement: Executable,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2313\u001b[39m     _add_event: Optional[Any] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2314\u001b[39m ) -> Result[Any]:\n\u001b[32m   2315\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Execute a SQL expression construct.\u001b[39;00m\n\u001b[32m   2316\u001b[39m \n\u001b[32m   2317\u001b[39m \u001b[33;03m    Returns a :class:`_engine.Result` object representing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2363\u001b[39m \n\u001b[32m   2364\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbind_arguments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbind_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2370\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_parent_execute_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_parent_execute_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2371\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_add_event\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_add_event\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2372\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\session.py:2251\u001b[39m, in \u001b[36mSession._execute_internal\u001b[39m\u001b[34m(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event, _scalar_result)\u001b[39m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m conn.scalar(\n\u001b[32m   2247\u001b[39m         statement, params \u001b[38;5;129;01mor\u001b[39;00m {}, execution_options=execution_options\n\u001b[32m   2248\u001b[39m     )\n\u001b[32m   2250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compile_state_cls:\n\u001b[32m-> \u001b[39m\u001b[32m2251\u001b[39m     result: Result[Any] = \u001b[43mcompile_state_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43morm_execute_statement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbind_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2260\u001b[39m     result = conn.execute(\n\u001b[32m   2261\u001b[39m         statement, params \u001b[38;5;129;01mor\u001b[39;00m {}, execution_options=execution_options\n\u001b[32m   2262\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\context.py:306\u001b[39m, in \u001b[36mAbstractORMCompileState.orm_execute_statement\u001b[39m\u001b[34m(cls, session, statement, params, execution_options, bind_arguments, conn)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34morm_execute_statement\u001b[39m(\n\u001b[32m    298\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    304\u001b[39m     conn,\n\u001b[32m    305\u001b[39m ) -> Result:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     result = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.orm_setup_cursor_result(\n\u001b[32m    310\u001b[39m         session,\n\u001b[32m    311\u001b[39m         statement,\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m         result,\n\u001b[32m    316\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1415\u001b[39m, in \u001b[36mConnection.execute\u001b[39m\u001b[34m(self, statement, parameters, execution_options)\u001b[39m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1416\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\elements.py:523\u001b[39m, in \u001b[36mClauseElement._execute_on_connection\u001b[39m\u001b[34m(self, connection, distilled_params, execution_options)\u001b[39m\n\u001b[32m    521\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m    522\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1637\u001b[39m, in \u001b[36mConnection._execute_clauseelement\u001b[39m\u001b[34m(self, elem, distilled_parameters, execution_options)\u001b[39m\n\u001b[32m   1625\u001b[39m compiled_cache: Optional[CompiledCacheType] = execution_options.get(\n\u001b[32m   1626\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompiled_cache\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.engine._compiled_cache\n\u001b[32m   1627\u001b[39m )\n\u001b[32m   1629\u001b[39m compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(\n\u001b[32m   1630\u001b[39m     dialect=dialect,\n\u001b[32m   1631\u001b[39m     compiled_cache=compiled_cache,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1635\u001b[39m     linting=\u001b[38;5;28mself\u001b[39m.dialect.compiler_linting | compiler.WARN_LINTING,\n\u001b[32m   1636\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1637\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1638\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[32m   1650\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_execute(\n\u001b[32m   1651\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1652\u001b[39m         elem,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1656\u001b[39m         ret,\n\u001b[32m   1657\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1842\u001b[39m, in \u001b[36mConnection._execute_context\u001b[39m\u001b[34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[39m\n\u001b[32m   1840\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exec_insertmany_context(dialect, context)\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1842\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1982\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1979\u001b[39m     result = context._setup_result_proxy()\n\u001b[32m   1981\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1982\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1983\u001b[39m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1986\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2354\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception\u001b[39m\u001b[34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[39m\n\u001b[32m   2352\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2353\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2354\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc_info[\u001b[32m1\u001b[39m].with_traceback(exc_info[\u001b[32m2\u001b[39m])\n\u001b[32m   2355\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2356\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._reentrant_error\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1963\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1961\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1962\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1963\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1964\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1967\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine._has_events:\n\u001b[32m   1968\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_cursor_execute(\n\u001b[32m   1969\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1970\u001b[39m         cursor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1974\u001b[39m         context.executemany,\n\u001b[32m   1975\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:943\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\encodings\\utf_8.py:15\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(input, errors)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m### Codec APIs\u001b[39;00m\n\u001b[32m     13\u001b[39m encode = codecs.utf_8_encode\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28minput\u001b[39m, errors=\u001b[33m'\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs.utf_8_decode(\u001b[38;5;28minput\u001b[39m, errors, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIncrementalEncoder\u001b[39;00m(codecs.IncrementalEncoder):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ab = run_basic_etl()\n",
    "ab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c54986bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>homepage_url</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>trending</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HubSpot Marketing AI</td>\n",
       "      <td>AI-powered marketing automation and content ge...</td>\n",
       "      <td>https://www.hubspot.com/products/marketing/art...</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>HubSpot</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jasper.ai</td>\n",
       "      <td>AI content creation platform for marketing cop...</td>\n",
       "      <td>https://www.jasper.ai/</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Jasper</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>2024-09-15</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Copy.ai</td>\n",
       "      <td>AI-powered copywriting assistant for marketing...</td>\n",
       "      <td>https://www.copy.ai/</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Copy.ai</td>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>2024-10-10</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MarketMuse</td>\n",
       "      <td>AI content planning and optimization for SEO a...</td>\n",
       "      <td>https://www.marketmuse.com/</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>MarketMuse</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2024-08-20</td>\n",
       "      <td>Medium</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Persado</td>\n",
       "      <td>AI language generation for marketing messaging...</td>\n",
       "      <td>https://www.persado.com/</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Persado</td>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>2024-07-30</td>\n",
       "      <td>Low</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Eleven Labs</td>\n",
       "      <td>AI voice synthesis and cloning platform</td>\n",
       "      <td>https://elevenlabs.io/</td>\n",
       "      <td>Others</td>\n",
       "      <td>Eleven Labs</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2024-08-20</td>\n",
       "      <td>High</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Murf AI</td>\n",
       "      <td>AI voice generation for content creation</td>\n",
       "      <td>https://murf.ai/</td>\n",
       "      <td>Others</td>\n",
       "      <td>Murf</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-07-30</td>\n",
       "      <td>Medium</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Synthesia</td>\n",
       "      <td>AI video creation with synthetic avatars</td>\n",
       "      <td>https://www.synthesia.io/</td>\n",
       "      <td>Others</td>\n",
       "      <td>Synthesia</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2024-08-15</td>\n",
       "      <td>High</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>DeepL</td>\n",
       "      <td>AI translation and language processing</td>\n",
       "      <td>https://www.deepl.com/</td>\n",
       "      <td>Others</td>\n",
       "      <td>DeepL</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2024-09-05</td>\n",
       "      <td>Medium</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>GitHub Copilot</td>\n",
       "      <td>AI code completion and programming assistant</td>\n",
       "      <td>https://github.com/features/copilot</td>\n",
       "      <td>Others</td>\n",
       "      <td>GitHub</td>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>2024-09-25</td>\n",
       "      <td>High</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                        description  \\\n",
       "0   HubSpot Marketing AI  AI-powered marketing automation and content ge...   \n",
       "1              Jasper.ai  AI content creation platform for marketing cop...   \n",
       "2                Copy.ai  AI-powered copywriting assistant for marketing...   \n",
       "3             MarketMuse  AI content planning and optimization for SEO a...   \n",
       "4                Persado  AI language generation for marketing messaging...   \n",
       "..                   ...                                                ...   \n",
       "67           Eleven Labs            AI voice synthesis and cloning platform   \n",
       "68               Murf AI           AI voice generation for content creation   \n",
       "69             Synthesia           AI video creation with synthetic avatars   \n",
       "70                 DeepL             AI translation and language processing   \n",
       "71        GitHub Copilot       AI code completion and programming assistant   \n",
       "\n",
       "                                         homepage_url   category       source  \\\n",
       "0   https://www.hubspot.com/products/marketing/art...  Marketing      HubSpot   \n",
       "1                              https://www.jasper.ai/  Marketing       Jasper   \n",
       "2                                https://www.copy.ai/  Marketing      Copy.ai   \n",
       "3                         https://www.marketmuse.com/  Marketing   MarketMuse   \n",
       "4                            https://www.persado.com/  Marketing      Persado   \n",
       "..                                                ...        ...          ...   \n",
       "67                             https://elevenlabs.io/     Others  Eleven Labs   \n",
       "68                                   https://murf.ai/     Others         Murf   \n",
       "69                          https://www.synthesia.io/     Others    Synthesia   \n",
       "70                             https://www.deepl.com/     Others        DeepL   \n",
       "71                https://github.com/features/copilot     Others       GitHub   \n",
       "\n",
       "    created_at  updated_at trending  id  \n",
       "0   2023-01-15  2024-10-01     High   0  \n",
       "1   2021-02-01  2024-09-15     High   1  \n",
       "2   2020-10-01  2024-10-10   Medium   2  \n",
       "3   2018-05-01  2024-08-20   Medium   3  \n",
       "4   2012-03-01  2024-07-30      Low   4  \n",
       "..         ...         ...      ...  ..  \n",
       "67  2022-01-01  2024-08-20     High  67  \n",
       "68  2020-01-01  2024-07-30   Medium  68  \n",
       "69  2017-01-01  2024-08-15     High  69  \n",
       "70  2017-01-01  2024-09-05   Medium  70  \n",
       "71  2021-06-01  2024-09-25     High  71  \n",
       "\n",
       "[72 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scraped['created_at'] = datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "# seed_df.drop(columns='id')\n",
    "\n",
    "seed_df['id'] = range(72\n",
    "                      )\n",
    "\n",
    "\n",
    "\n",
    "seed_df.drop(columns='id')\n",
    "\n",
    "seed_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-directory-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
