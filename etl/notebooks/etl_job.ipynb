{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675d500a",
   "metadata": {},
   "source": [
    "## TASKS\n",
    "```\n",
    "DESCRIPTION                             STATUS \n",
    "- Load seed data into DB.               done\n",
    "- Clean and transform scraped data      done\n",
    "- Create an idempotent etl job          ongoing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd5008c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c588289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f8b61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\20250601_213652_ai_tools_scraped.csv\"\n",
    "seed_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\seeded_ai_agents.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a81ac",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30430eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(source_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Data Path\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises error for unsupported data type.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas Dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ext = Path(source_path).suffix\n",
    "        if ext == \".csv\":\n",
    "            return pd.read_csv(source_path)\n",
    "        elif ext == \".json\":\n",
    "            return pd.read_json(source_path)\n",
    "        elif ext == \".parquet\":\n",
    "            return pd.read_parquet(source_path)\n",
    "        logger.info(\"Data successfully read!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}. Unsupported file format! Use csv, json or parquet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a875aff",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a69a1ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4233 entries, 0 to 4232\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name         4233 non-null   object\n",
      " 1   description  4233 non-null   object\n",
      " 2   url          4231 non-null   object\n",
      " 3   source       4233 non-null   object\n",
      " 4   category     4232 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 165.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72 entries, 0 to 71\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   name          72 non-null     object\n",
      " 1   description   72 non-null     object\n",
      " 2   homepage_url  72 non-null     object\n",
      " 3   category      72 non-null     object\n",
      " 4   source        72 non-null     object\n",
      " 5   created_at    72 non-null     object\n",
      " 6   updated_at    72 non-null     object\n",
      " 7   trending      72 non-null     object\n",
      "dtypes: object(8)\n",
      "memory usage: 4.6+ KB\n"
     ]
    }
   ],
   "source": [
    "scraped_df = read_data(scraped_data_source)\n",
    "seed_df = read_data(seed_data_source)\n",
    "\n",
    "scraped_df.info()\n",
    "seed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fa97d",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Cleaning \"tags\" column\n",
    "- separate the values in the list and choose the unique tag. \n",
    "- Each tag must be just a value. (i.e list of len 1)\n",
    "    + no # in value \n",
    "    + no duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9b49919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = df.drop(columns=[col for col in ['pricing', 'page'] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        logger.info(\"Columns dropped and null values dropped.\",\n",
    "                    extra={\n",
    "                     \"Cols dropped\": ['pricing', 'page'],\n",
    "                     \"Null Values Dropped\": len(df) - len(new_df)\n",
    "                    }\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised {e}! Is the input a dataframe? Use a pandas dataframe.\",  exc_info=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_hashtags(tags):\n",
    "    try:\n",
    "        if isinstance(tags, list):\n",
    "            clean = [tag for tag in tags if '#' not in tag]\n",
    "        elif isinstance(tags, str):\n",
    "            clean = [tags] if \"#\" not in tags else []\n",
    "        else:\n",
    "            clean = []\n",
    "        clean = ','.join(clean)\n",
    "\n",
    "        if len(clean) < 4:\n",
    "            clean = clean.upper()\n",
    "        else:\n",
    "            clean = clean.lower().capitalize()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at tags column cleaning {e}! Use tags column.\",  exc_info=True)\n",
    "    return clean\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    try:\n",
    "        # df = baseline_cleaning(df=scraped_df)\n",
    "        df = df.drop(columns=[col for col in ['pricing', 'page'] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "        if 'tags' in df.columns:\n",
    "            new_df['tags'] = new_df['tags'].apply(remove_hashtags)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        logger.info(\"Columns dropped and null values dropped.\",\n",
    "                    extra={\n",
    "                     \"Cols dropped\": ['pricing', 'page'],\n",
    "                     \"Null Values Dropped\": len(df) - len(new_df)\n",
    "                    }\n",
    "                    )\n",
    "        logger.info(\"Tags Column Successfully cleaned.\")\n",
    "        logger.info(\"Data successfully cleaned!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at full cleaning process: {e}!\",  exc_info=True)\n",
    "    return new_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfcfc1",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3606b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_created_at(filepath: str) -> str:\n",
    "    try:\n",
    "        created_timestamp = os.path.getctime(filepath)\n",
    "        created_date = datetime.fromtimestamp(created_timestamp)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised: {e}!\", exc_info=True)\n",
    "    return created_date.strftime(\"%Y-%M-%d\")\n",
    "\n",
    "\n",
    "def transform_data(df: pd.DataFrame, source = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        created_day = get_created_at(scraped_data_source)\n",
    "        if 'source' in df.columns:\n",
    "            if df['source'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['source'] = source\n",
    "        else:\n",
    "            df['source'] = source\n",
    "\n",
    "\n",
    "        if 'created_at' in df.columns:\n",
    "            if df['created_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['created_at'] = created_day\n",
    "        else:\n",
    "            df['created_at'] = created_day\n",
    "        if 'updated_at' in df.columns:\n",
    "            if df['updated_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['updated_at'] = None\n",
    "        else:\n",
    "            df['updated_at'] = None\n",
    "        \n",
    "\n",
    "        if 'trending' not in df.columns:\n",
    "            df['trending'] = None\n",
    "            df['trending'] = df['trending'].notna().astype(bool)\n",
    "        else:\n",
    "            df[\"trending\"] = df[\"trending\"].apply(\n",
    "                lambda x: False if x == 'Low' else True\n",
    "                )\n",
    "            df['trending'] = df['trending'].astype(bool)\n",
    "    \n",
    "        trans_df = df.rename(columns={'url': 'homepage_url', 'tags': 'category'})\n",
    "        \n",
    "        trans_df['created_at'] = pd.to_datetime(trans_df['created_at'], format=\"%Y-%M-%d\", errors=\"coerce\")\n",
    "        trans_df['updated_at'] = pd.to_datetime(trans_df['updated_at'], format=\"%Y-%M-%d\", errors=\"coerce\")\n",
    "\n",
    "        logger.info(\"Data successfully transformed!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at transformation: {e}!\", exc_info=True)\n",
    "    return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dba06c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_dfs(new_df, existing_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merging DFs to extract unique ai_tools\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DF with unique Ai tools\n",
    "    \"\"\"\n",
    "    try:\n",
    "        merged_df = pd.merge(existing_df, new_df, how=\"outer\", suffixes=\"_existing\")\n",
    "        merged_df.drop_duplicates(subset=[\n",
    "            \"name\", \"homepage_url\"\n",
    "            ], inplace=True)\n",
    "        merged_df = merged_df.reset_index(drop=True)\n",
    "        logger.info(\"Existing DB Data and Scraped Data successfully merged!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error merging DFs: %s\", e, exc_info=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f82458",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bca5e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_etl() -> pd.DataFrame:\n",
    "    # Extract\n",
    "    scraped_df = read_data(scraped_data_source)\n",
    "\n",
    "    # Clean\n",
    "    clean_scraped_df = clean_data(scraped_df)\n",
    "\n",
    "    # Transform\n",
    "    trans_scraped_df = transform_data(\n",
    "        clean_scraped_df, source=\"https://aitoolsdirectory.com/\"\n",
    "    )\n",
    "\n",
    "    return trans_scraped_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff6a3101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 22:03:55,423 - INFO - Columns dropped and null values dropped.\n",
      "2025-06-01 22:03:55,431 - INFO - Tags Column Successfully cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 22:03:55,471 - INFO - Data successfully cleaned!\n",
      "2025-06-01 22:03:55,637 - INFO - Data successfully transformed!\n"
     ]
    }
   ],
   "source": [
    "comp_df = run_basic_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c39c5f",
   "metadata": {},
   "source": [
    "## Idempotent ETL Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75079cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ai_tools ETL Local DB Setup and data upload\n",
    "\n",
    "Name: Arowosegbe Victor Iyanuoluwa\\n\n",
    "Email: Iyanuvicky@gmail.com\\n\n",
    "GitHub: https://github.com/Iyanuvicky22/projects\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    Column,\n",
    "    String,\n",
    "    Text,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    "    func,\n",
    "    Integer,\n",
    ")\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"\n",
    "    Database connector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(DB_URL)\n",
    "        Session = sessionmaker(bind=engine, autoflush=False)\n",
    "        Base.metadata.create_all(engine)\n",
    "        logger.info(\"Database succesfully connected to.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(\"Databse connection error: %s\", e, exc_info=True)\n",
    "    return Session, engine\n",
    "\n",
    "\n",
    "class Agent(Base):\n",
    "    \"\"\"\n",
    "    Agents table model creation\n",
    "    Args:\n",
    "        Base (): SQLAlchemy Base model\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = \"agents\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    name = Column(String, nullable=False, index=True)\n",
    "    description = Column(Text)\n",
    "    homepage_url = Column(String)\n",
    "    category = Column(String)\n",
    "    source = Column(String)\n",
    "    trending = Column(Boolean, default=False)\n",
    "    created_at = Column(DateTime, server_default=func.now(), nullable=False)\n",
    "    updated_at = Column(\n",
    "        DateTime, server_default=func.now(), onupdate=func.now(), nullable=False\n",
    "    )\n",
    "\n",
    "\n",
    "def load_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to load data into the Database (PostgreSQL).\n",
    "    Args:\n",
    "        df (pd.DataFrame): Cleaned and Transformed data to be loaded.\n",
    "    \"\"\"\n",
    "    Session, engine = connect_db()\n",
    "    data = df\n",
    "\n",
    "    def change_to_none(row):\n",
    "        row.replace(pd.NaT, '')\n",
    "        return row\n",
    "\n",
    "\n",
    "    with Session.begin() as session:\n",
    "        for _, row in data.iterrows():\n",
    "            ai_tool = session.query(Agent).filter_by(name=str(row[\"name\"])).first()\n",
    "\n",
    "            if not ai_tool:\n",
    "                agent = Agent(\n",
    "                    name=str(row[\"name\"]),\n",
    "                    description=str(row[\"description\"]),\n",
    "                    homepage_url=row[\"homepage_url\"],\n",
    "                    category=row[\"category\"],\n",
    "                    source=row[\"source\"],\n",
    "                    trending=row[\"trending\"],\n",
    "                    created_at=row[\"created_at\"],\n",
    "                    updated_at=row[\"updated_at\"],\n",
    "                )\n",
    "                session.add(agent)\n",
    "        session.commit()\n",
    "        logger.info(\"Data successfully loaded in database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25f49dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\20250601_213652_ai_tools_scraped.csv\"\n",
    "\n",
    "def trans_load_seed_df():\n",
    "    \"\"\"\n",
    "    Function to Load Seed Data into Database\n",
    "    \"\"\"\n",
    "    data = read_data(source_path=path)\n",
    "\n",
    "    clean_df = clean_data(data)\n",
    "\n",
    "    trans_seed_df = transform_data(df=clean_df)\n",
    "\n",
    "    # load_data(trans_seed_df)\n",
    "\n",
    "    return trans_seed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc723e",
   "metadata": {},
   "source": [
    "## Checking ETL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ecadfc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 22:04:00,715 - INFO - Columns dropped and null values dropped.\n",
      "2025-06-01 22:04:00,733 - INFO - Tags Column Successfully cleaned.\n",
      "2025-06-01 22:04:00,748 - INFO - Data successfully cleaned!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 22:04:00,838 - INFO - Data successfully transformed!\n"
     ]
    }
   ],
   "source": [
    "ab = trans_load_seed_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8f992d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4230 entries, 0 to 4229\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   name          4230 non-null   object        \n",
      " 1   description   4230 non-null   object        \n",
      " 2   homepage_url  4230 non-null   object        \n",
      " 3   source        4230 non-null   object        \n",
      " 4   category      4230 non-null   object        \n",
      " 5   created_at    4230 non-null   datetime64[ns]\n",
      " 6   updated_at    0 non-null      datetime64[ns]\n",
      " 7   trending      4230 non-null   bool          \n",
      "dtypes: bool(1), datetime64[ns](2), object(5)\n",
      "memory usage: 235.6+ KB\n"
     ]
    }
   ],
   "source": [
    "ab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457eeb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>homepage_url</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X-Design</td>\n",
       "      <td>Specialist AI tool for product photography. Up...</td>\n",
       "      <td>https://www.x-design.com/</td>\n",
       "      <td>Image Editing</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KIVA</td>\n",
       "      <td>KIVA is an AI SEO tool that automates keyword ...</td>\n",
       "      <td>https://wellows.com/kiva/</td>\n",
       "      <td>SEO</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Parliant</td>\n",
       "      <td>Parliant AI offers AI-driven conversational su...</td>\n",
       "      <td>https://www.parliant.ai/</td>\n",
       "      <td>Business Intelligence</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Galaxy AI</td>\n",
       "      <td>Your go-to hub for AI tools. This all-in-one A...</td>\n",
       "      <td>https://link.aitoolsdirectory.com/galaxy</td>\n",
       "      <td>Productivity</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dropmagic</td>\n",
       "      <td>Build Shopify stores from AliExpress or produc...</td>\n",
       "      <td>https://link.aitoolsdirectory.com/dropmagic</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>https://aitoolsdirectory.com</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8066</th>\n",
       "      <td>Photofeeler</td>\n",
       "      <td>Photofeeler provides unbiased photo feedback t...</td>\n",
       "      <td>https://www.photofeeler.com?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8067</th>\n",
       "      <td>Thundr.tv</td>\n",
       "      <td>Anonymous, AI-moderated video and text chat pl...</td>\n",
       "      <td>https://www.thundr.tv?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8068</th>\n",
       "      <td>ShowZone</td>\n",
       "      <td>MLB The Show tools and resources for strategic...</td>\n",
       "      <td>https://showzone.gg?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8069</th>\n",
       "      <td>Dover</td>\n",
       "      <td>Dover helps startups hire top talent with frac...</td>\n",
       "      <td>https://www.dover.com/trial?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8070</th>\n",
       "      <td>DuckDuckGo</td>\n",
       "      <td>Privacy-focused search engine and browser with...</td>\n",
       "      <td>https://duck.ai?utm_source=toolify</td>\n",
       "      <td>https://www.toolify.ai</td>\n",
       "      <td>Other</td>\n",
       "      <td>2025-01-29 00:18:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6895 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                                        description  \\\n",
       "0        X-Design  Specialist AI tool for product photography. Up...   \n",
       "1            KIVA  KIVA is an AI SEO tool that automates keyword ...   \n",
       "2        Parliant  Parliant AI offers AI-driven conversational su...   \n",
       "3       Galaxy AI  Your go-to hub for AI tools. This all-in-one A...   \n",
       "4       Dropmagic  Build Shopify stores from AliExpress or produc...   \n",
       "...           ...                                                ...   \n",
       "8066  Photofeeler  Photofeeler provides unbiased photo feedback t...   \n",
       "8067    Thundr.tv  Anonymous, AI-moderated video and text chat pl...   \n",
       "8068     ShowZone  MLB The Show tools and resources for strategic...   \n",
       "8069        Dover  Dover helps startups hire top talent with frac...   \n",
       "8070   DuckDuckGo  Privacy-focused search engine and browser with...   \n",
       "\n",
       "                                        homepage_url                category  \\\n",
       "0                          https://www.x-design.com/           Image Editing   \n",
       "1                          https://wellows.com/kiva/                     SEO   \n",
       "2                           https://www.parliant.ai/   Business Intelligence   \n",
       "3           https://link.aitoolsdirectory.com/galaxy            Productivity   \n",
       "4        https://link.aitoolsdirectory.com/dropmagic               Marketing   \n",
       "...                                              ...                     ...   \n",
       "8066  https://www.photofeeler.com?utm_source=toolify  https://www.toolify.ai   \n",
       "8067        https://www.thundr.tv?utm_source=toolify  https://www.toolify.ai   \n",
       "8068          https://showzone.gg?utm_source=toolify  https://www.toolify.ai   \n",
       "8069  https://www.dover.com/trial?utm_source=toolify  https://www.toolify.ai   \n",
       "8070              https://duck.ai?utm_source=toolify  https://www.toolify.ai   \n",
       "\n",
       "                            source          created_at updated_at  trending  \n",
       "0     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "1     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "2     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "3     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "4     https://aitoolsdirectory.com 2025-01-29 00:18:00        NaT     False  \n",
       "...                            ...                 ...        ...       ...  \n",
       "8066                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "8067                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "8068                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "8069                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "8070                         Other 2025-01-29 00:18:00        NaT     False  \n",
       "\n",
       "[6895 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.drop_duplicates(subset=['homepage_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9489ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaT\n"
     ]
    }
   ],
   "source": [
    "ab.duplicated(subset=['name', 'homepage_url']).value_counts()\n",
    "\n",
    "ab[ab.duplicated(subset=['name'])]\n",
    "\n",
    "def turn_to_none(col):\n",
    "    for row in col:\n",
    "        if row == pd.NaT:\n",
    "            row = None\n",
    "        else: \n",
    "            row = row\n",
    "    return row\n",
    "\n",
    "ac = turn_to_none(ab['updated_at'])\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d23295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 21:32:23,276 - ERROR - Databse connection error: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 145, in __init__\n",
      "    self._dbapi_connection = engine.raw_connection()\n",
      "                             ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3297, in raw_connection\n",
      "    return self.pool.connect()\n",
      "           ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 449, in connect\n",
      "    return _ConnectionFairy._checkout(self)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 1264, in _checkout\n",
      "    fairy = _ConnectionRecord.checkout(pool)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 713, in checkout\n",
      "    rec = pool._do_get()\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py\", line 179, in _do_get\n",
      "    with util.safe_reraise():\n",
      "         ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 224, in __exit__\n",
      "    raise exc_value.with_traceback(exc_tb)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py\", line 177, in _do_get\n",
      "    return self._create_connection()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 390, in _create_connection\n",
      "    return _ConnectionRecord(self)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 675, in __init__\n",
      "    self.__connect()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 901, in __connect\n",
      "    with util.safe_reraise():\n",
      "         ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 224, in __exit__\n",
      "    raise exc_value.with_traceback(exc_tb)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 897, in __connect\n",
      "    self.dbapi_connection = connection = pool._invoke_creator(self)\n",
      "                                         ~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py\", line 646, in connect\n",
      "    return dialect.connect(*cargs, **cparams)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py\", line 625, in connect\n",
      "    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py\", line 135, in connect\n",
      "    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n",
      "psycopg2.OperationalError: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\APIN-PC\\AppData\\Local\\Temp\\ipykernel_6896\\203178352.py\", line 45, in connect_db\n",
      "    Base.metadata.create_all(engine)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py\", line 5924, in create_all\n",
      "    bind._run_ddl_visitor(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~^\n",
      "        ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3247, in _run_ddl_visitor\n",
      "    with self.begin() as conn:\n",
      "         ~~~~~~~~~~^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 141, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3237, in begin\n",
      "    with self.connect() as conn:\n",
      "         ~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3273, in connect\n",
      "    return self._connection_cls(self)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 147, in __init__\n",
      "    Connection._handle_dbapi_exception_noconnection(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        err, dialect, engine\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 2436, in _handle_dbapi_exception_noconnection\n",
      "    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 145, in __init__\n",
      "    self._dbapi_connection = engine.raw_connection()\n",
      "                             ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 3297, in raw_connection\n",
      "    return self.pool.connect()\n",
      "           ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 449, in connect\n",
      "    return _ConnectionFairy._checkout(self)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 1264, in _checkout\n",
      "    fairy = _ConnectionRecord.checkout(pool)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 713, in checkout\n",
      "    rec = pool._do_get()\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py\", line 179, in _do_get\n",
      "    with util.safe_reraise():\n",
      "         ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 224, in __exit__\n",
      "    raise exc_value.with_traceback(exc_tb)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py\", line 177, in _do_get\n",
      "    return self._create_connection()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 390, in _create_connection\n",
      "    return _ConnectionRecord(self)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 675, in __init__\n",
      "    self.__connect()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 901, in __connect\n",
      "    with util.safe_reraise():\n",
      "         ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 224, in __exit__\n",
      "    raise exc_value.with_traceback(exc_tb)\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 897, in __connect\n",
      "    self.dbapi_connection = connection = pool._invoke_creator(self)\n",
      "                                         ~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py\", line 646, in connect\n",
      "    return dialect.connect(*cargs, **cparams)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py\", line 625, in connect\n",
      "    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py\", line 135, in connect\n",
      "    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n",
      "sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "connection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3297\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3276\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3277\u001b[39m \n\u001b[32m   3278\u001b[39m \u001b[33;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3295\u001b[39m \n\u001b[32m   3296\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    443\u001b[39m \n\u001b[32m    444\u001b[39m \u001b[33;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \n\u001b[32m    448\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:713\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:675\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:901\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:897\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    896\u001b[39m \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    644\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:625\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    624\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m         conn.commit()\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m db_df\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m db_df = \u001b[43mfetch_db_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mfetch_db_records\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_db_records\u001b[39m():\n\u001b[32m      4\u001b[39m     session, engine = connect_db()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m      7\u001b[39m         db_df = pd.read_sql(\u001b[33m\"\u001b[39m\u001b[33mSELECT * from agents\u001b[39m\u001b[33m\"\u001b[39m, con=conn)\n\u001b[32m      8\u001b[39m         conn.commit()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3273\u001b[39m, in \u001b[36mEngine.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3250\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Connection:\n\u001b[32m   3251\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[32m   3252\u001b[39m \n\u001b[32m   3253\u001b[39m \u001b[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3270\u001b[39m \n\u001b[32m   3271\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:147\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = engine.raw_connection()\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         \u001b[43mConnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2436\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception_noconnection\u001b[39m\u001b[34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[39m\n\u001b[32m   2434\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2435\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2437\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2438\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:145\u001b[39m, in \u001b[36mConnection.__init__\u001b[39m\u001b[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m         \u001b[38;5;28mself\u001b[39m._dbapi_connection = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect.loaded_dbapi.Error \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    147\u001b[39m         Connection._handle_dbapi_exception_noconnection(\n\u001b[32m    148\u001b[39m             err, dialect, engine\n\u001b[32m    149\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3297\u001b[39m, in \u001b[36mEngine.raw_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m   3276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[32m   3277\u001b[39m \n\u001b[32m   3278\u001b[39m \u001b[33;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3295\u001b[39m \n\u001b[32m   3296\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[39m, in \u001b[36mPool.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> PoolProxiedConnection:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[32m    443\u001b[39m \n\u001b[32m    444\u001b[39m \u001b[33;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \n\u001b[32m    448\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001b[39m, in \u001b[36m_ConnectionFairy._checkout\u001b[39m\u001b[34m(cls, pool, threadconns, fairy)\u001b[39m\n\u001b[32m   1256\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_checkout\u001b[39m(\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1261\u001b[39m     fairy: Optional[_ConnectionFairy] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1262\u001b[39m ) -> _ConnectionFairy:\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m         fairy = \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1267\u001b[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:713\u001b[39m, in \u001b[36m_ConnectionRecord.checkout\u001b[39m\u001b[34m(cls, pool)\u001b[39m\n\u001b[32m    711\u001b[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     rec = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    716\u001b[39m     dbapi_connection = rec.get_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_connection()\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dec_overflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[39m, in \u001b[36mQueuePool._do_get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inc_overflow():\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m util.safe_reraise():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[39m, in \u001b[36mPool._create_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ConnectionPoolEntry:\n\u001b[32m    388\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:675\u001b[39m, in \u001b[36m_ConnectionRecord.__init__\u001b[39m\u001b[34m(self, pool, connect)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28mself\u001b[39m.__pool = pool\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_callback = deque()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:901\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError on connect(): \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:897\u001b[39m, in \u001b[36m_ConnectionRecord.__connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;28mself\u001b[39m.starttime = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbapi_connection = connection = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    898\u001b[39m     pool.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, connection)\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mself\u001b[39m.fresh = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[39m, in \u001b[36mcreate_engine.<locals>.connect\u001b[39m\u001b[34m(connection_record)\u001b[39m\n\u001b[32m    643\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    644\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:625\u001b[39m, in \u001b[36mDefaultDialect.connect\u001b[39m\u001b[34m(self, *cargs, **cparams)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001b[32m    624\u001b[39m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaded_dbapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    134\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    137\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "def fetch_db_records():\n",
    "    session, engine = connect_db()\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        db_df = pd.read_sql(\"SELECT * from agents\", con=conn)\n",
    "        conn.commit()\n",
    "    return db_df\n",
    "\n",
    "db_df = fetch_db_records()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4822601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trending\n",
       "True     60\n",
       "False    12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_df.trending.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d97d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc3718cf",
   "metadata": {},
   "source": [
    "## Functions to Work On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_check(new_df, existing_df):\n",
    "    # Merge and check for differences\n",
    "    merged = new_df.merge(\n",
    "        existing_df, on=[\"name\", \"homepage_url\"], how=\"left\", suffixes=(\"\", \"_existing\")\n",
    "    )\n",
    "    changed = merged[\n",
    "        (merged[\"name\"] != merged[\"name_existing\"])\n",
    "        | (merged[\"homepage_url\"] != merged[\"homepage_url_existing\"])\n",
    "    ]\n",
    "    return changed[new_df.columns] \n",
    "\n",
    "\n",
    "\n",
    "def upsert_records(conn, df):\n",
    "    cursor = conn.cursor()\n",
    "    for _, row in df.iterrows():\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO agents (name, homepage_url, email, phone)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "            ON CONFLICT(name, homepage_url)\n",
    "            DO UPDATE SET email=excluded.email, phone=excluded.phone\n",
    "        \"\"\",\n",
    "            (row[\"name\"], row[\"homepage_url\"], row[\"email\"], row[\"phone\"]),\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def etl_job(source_path):\n",
    "    df = read_data(source_path)\n",
    "\n",
    "    if needs_transformation(df):\n",
    "        df = transform_data(df)\n",
    "\n",
    "    with engine.connect(\"agents.db\") as conn:\n",
    "        existing_df = fetch_existing_records(conn)\n",
    "        delta_df = delta_check(df, existing_df)\n",
    "\n",
    "        if not delta_df.empty:\n",
    "            upsert_records(conn, delta_df)\n",
    "            print(f\"Upserted {len(delta_df)} records.\")\n",
    "        else:\n",
    "            print(\"No changes detected. Idempotent run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607bc10d",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Pending tasks\n",
    "- Load Seed data separately.    ```done```\n",
    "- Load scraped data and check for duplicates with name(lower) and homepage_url.\n",
    "- Write tests to check for:\n",
    "    + test for duplicates.\n",
    "    + test for invalid rows.\n",
    "    + test for correct upserts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d54bb",
   "metadata": {},
   "source": [
    "## FUNCTIONALITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b4ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7b129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='.env')\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
    "s3 = boto3.client(\"s3\", region_name=AWS_REGION,\n",
    "                  aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                  aws_secret_access_key=AWS_SECRET_KEY)\n",
    "\n",
    "bucket_name = 'scraped-ai-agent'\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    filemode='w',\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fa685",
   "metadata": {},
   "source": [
    "### Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a761a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(source_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Data Path\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises error for unsupported data type.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas Dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ext = Path(source_path).suffix\n",
    "        if ext == \".csv\":\n",
    "            return pd.read_csv(source_path)\n",
    "        elif ext == \".json\":\n",
    "            return pd.read_json(source_path)\n",
    "        elif ext == \".parquet\":\n",
    "            return pd.read_parquet(source_path)\n",
    "        logger.info(\"Data successfully read!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"Error raised at data loading. Unsupported file format! Use csv, json or parquet: %s\",\n",
    "            e,\n",
    "            exc_info=True,\n",
    "        )\n",
    "\n",
    "\n",
    "def remove_hashtags(tags):\n",
    "    \"\"\"\n",
    "    Method to clean category column from \"https://aitoolsdirectory.com/\"\n",
    "    Args:\n",
    "        tags (Series): Column to be cleaned\n",
    "\n",
    "    Returns:\n",
    "        Series: Cleaned column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(tags, list):\n",
    "            clean = [tag for tag in tags if \"#\" not in tag]\n",
    "        elif isinstance(tags, str):\n",
    "            clean = [tags] if \"#\" not in tags else []\n",
    "        else:\n",
    "            clean = []\n",
    "        clean = \",\".join(clean)\n",
    "\n",
    "        if len(clean) < 4:\n",
    "            clean = clean.upper()\n",
    "        else:\n",
    "            clean = clean.lower().capitalize()\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error Raised at tags column cleaning:  %s\", e, exc_info=True)\n",
    "    return clean\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Custom function to clean scraped/manual ai_tools dataset\n",
    "    Args:\n",
    "        df (pd.DataFrame): Scraped/manually created ai_tools dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned ai_tools dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.drop(columns=[col for col in [\"pricing\", \"page\"] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "        if \"category\" in df.columns:\n",
    "            new_df[\"category\"] = new_df[\"category\"].apply(remove_hashtags)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        logger.info(\n",
    "            \"Columns dropped and null values dropped.\",\n",
    "            extra={\n",
    "                \"Cols dropped\": [\"pricing\", \"page\"],\n",
    "                \"Null Values Dropped\": len(df) - len(new_df),\n",
    "            },\n",
    "        )\n",
    "        logger.info(\"Tags Column Successfully cleaned.\")\n",
    "        logger.info(\"Data successfully cleaned!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error Raised at full cleaning process: %s\", e, exc_info=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def get_created_at(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracting file creation date\n",
    "    Args:\n",
    "        filepath (str): filepath\n",
    "\n",
    "    Returns:\n",
    "        str: creation time in strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        created_timestamp = os.path.getctime(filepath)\n",
    "        created_date = datetime.fromtimestamp(created_timestamp)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised: {e}!\", exc_info=True)\n",
    "    return created_date.strftime(\"%Y-%M-%d\")\n",
    "\n",
    "\n",
    "def transform_data(df: pd.DataFrame, source=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Custom function to transform any scraped and cleaned ai_tools dataset.\n",
    "    Args:\n",
    "        df (pd.DataFrame): Clean ai_tools dataset\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed ai_tools dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # created_day = get_created_at(scraped_data_source)\n",
    "        if \"source\" in df.columns:\n",
    "            if df[\"source\"] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df[\"source\"] = source\n",
    "        else:\n",
    "            df[\"source\"] = source\n",
    "\n",
    "        if \"created_at\" in df.columns:\n",
    "            if df['created_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df[\"created_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            df[\"created_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        df[\"updated_at\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        if \"trending\" not in df.columns:\n",
    "            df[\"trending\"] = 0\n",
    "            # df[\"trending\"] = df[\"trending\"].notna().astype(bool)\n",
    "        else:\n",
    "            df[\"trending\"] = df[\"trending\"].apply(\n",
    "                lambda x: 0 if x == \"Low\" else 1\n",
    "            )\n",
    "        df[\"trending\"] = 0\n",
    "\n",
    "        trans_df = df.rename(columns={\"url\": \"homepage_url\", \"tags\": \"category\"})\n",
    "\n",
    "        trans_df[\"created_at\"] = pd.to_datetime(\n",
    "            trans_df[\"created_at\"], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    "        )\n",
    "        trans_df[\"updated_at\"] = pd.to_datetime(\n",
    "            trans_df[\"updated_at\"], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    "        )\n",
    "        trans_df = trans_df.drop_duplicates(subset=['name', 'homepage_url'])\n",
    "\n",
    "        logger.info(\"Data successfully transformed!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error Raised at transformation: %s\", e, exc_info=True)\n",
    "    return trans_df\n",
    "\n",
    "\n",
    "def merging_dfs(new_df, existing_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merging DFs to extract unique ai_tools\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DF with unique Ai tools\n",
    "    \"\"\"\n",
    "    try:\n",
    "        merged_df = pd.merge(new_df, existing_df, how=\"outer\")\n",
    "        merged_df.drop_duplicates(subset=[\n",
    "            \"name\", \"homepage_url\"\n",
    "            ], inplace=True)\n",
    "        merged_df = merged_df.reset_index(drop=True)\n",
    "        logger.info(\"Existing DB Data and Scraped Data successfully merged!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error merging DFs: %s\", e, exc_info=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def fetch_db_records():\n",
    "    session, engine = connect_db()\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        db_df = pd.read_sql(\"SELECT * from agents\", con=conn)\n",
    "        conn.commit()\n",
    "    return db_df\n",
    "\n",
    "\n",
    "def dump_raw_data_to_s3(file_path: str):\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, f\"{os.path.basename(file_path)}\")\n",
    "        logger.info(f\"Successfully upload to s3://{bucket_name}/{file_path}\")\n",
    "        os.remove(file_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Uploading failed: {e}\")\n",
    "\n",
    "\n",
    "def fetch_latest_csv_from_s3(download_dir='downloads'):\n",
    "\n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        contents = response.get('Contents', [])\n",
    "\n",
    "        # Filter for CSV files and sort by last modified time\n",
    "        csv_files = [obj for obj in contents if obj['Key'].endswith('.csv')]\n",
    "        if not csv_files:\n",
    "            logger.info(\"âŒ No CSV files found.\")\n",
    "            return None\n",
    "\n",
    "        latest_file = max(csv_files, key=lambda x: x['LastModified'])\n",
    "        latest_key = latest_file['Key']\n",
    "        filename = os.path.basename(latest_key)\n",
    "        local_path = os.path.join(download_dir, filename)\n",
    "\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        s3.download_file(bucket_name, latest_key, local_path)\n",
    "\n",
    "        logger.info(f\"âœ… Downloaded latest CSV: {latest_key} â†’ {local_path}\")\n",
    "        return local_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Failed to fetch from S3: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955a812",
   "metadata": {},
   "source": [
    "### Models Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    Column,\n",
    "    String,\n",
    "    Text,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    "    func,\n",
    "    Integer,\n",
    ")\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"\n",
    "    Database connector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(DB_URL)\n",
    "        Session = sessionmaker(bind=engine, autoflush=False)\n",
    "        Base.metadata.create_all(engine)\n",
    "        logger.info(\"Database succesfully connected to.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(\"Databse connection error: %s\", e, exc_info=True)\n",
    "    return Session, engine\n",
    "\n",
    "\n",
    "class AiAgent(Base):\n",
    "    \"\"\"\n",
    "    Agents table model creation\n",
    "    Args:\n",
    "        Base (): SQLAlchemy Base model\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = \"ai_agents\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    name = Column(String, nullable=False, index=True)\n",
    "    description = Column(Text)\n",
    "    homepage_url = Column(String)\n",
    "    category = Column(String)\n",
    "    source = Column(String)\n",
    "    trending = Column(Boolean, default=False)\n",
    "    created_at = Column(DateTime, server_default=func.now(), nullable=False)\n",
    "    updated_at = Column(\n",
    "        DateTime, server_default=func.now(), onupdate=func.now(), nullable=False)\n",
    "\n",
    "\n",
    "def load_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to load data into the Database (PostgreSQL).\n",
    "    Args:\n",
    "        df (pd.DataFrame): Cleaned and Transformed data to be loaded.\n",
    "    \"\"\"\n",
    "    Session, engine = connect_db()\n",
    "    data = df\n",
    "\n",
    "    with Session.begin() as session:\n",
    "        try:\n",
    "            for _, row in data.iterrows():\n",
    "                ai_tool = session.query(AiAgent).filter_by(\n",
    "                    name=str(row[\"name\"]),\n",
    "                    homepage_url=row['homepage_url']                                       \n",
    "                    ).first()\n",
    "\n",
    "                if ai_tool:\n",
    "                    ai_tool.description = row.get(\"description\", ai_tool.description)\n",
    "                    ai_tool.category = row.get(\"category\", ai_tool.category)\n",
    "                    ai_tool.source = row.get(\"source\", ai_tool.source)\n",
    "                    ai_tool.updated_at = row.get(\"updated_at\", ai_tool.updated_at)\n",
    "                else:\n",
    "                    ai_tool = AiAgent(\n",
    "                        name=str(row[\"name\"]),\n",
    "                        description=str(row[\"description\"]),\n",
    "                        homepage_url=row[\"homepage_url\"],\n",
    "                        category=row[\"category\"],\n",
    "                        source=row[\"source\"],\n",
    "                        trending=row[\"trending\"],\n",
    "                        created_at=row[\"created_at\"],\n",
    "                        updated_at=row[\"updated_at\"],\n",
    "                    )\n",
    "                    session.add(ai_tool)\n",
    "            session.commit()\n",
    "            logger.info(\"Data successfully loaded in database!\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Data upload failed: %s\", e, exc_info=True)\n",
    "            session.rollback()\n",
    "        finally:\n",
    "            session.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075269e5",
   "metadata": {},
   "source": [
    "### ETL Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0566fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_etl() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Basic ETL Job.\n",
    "    Returns:\n",
    "        pd.DataFrame: Ai tools data to run etl job on.\n",
    "    \"\"\"\n",
    "    # download latest file from s3\n",
    "    scraped_data_source = fetch_latest_csv_from_s3()\n",
    "\n",
    "    scraped_df = read_data(scraped_data_source)\n",
    "\n",
    "    clean_scraped_df = clean_data(scraped_df)\n",
    "\n",
    "    trans_scraped_df = transform_data(clean_scraped_df)\n",
    "    \n",
    "    existing_db_df = fetch_db_records()\n",
    "\n",
    "    final_df = merging_dfs(trans_scraped_df, existing_db_df)\n",
    "\n",
    "    load_data(final_df)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "801221ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 14:54:43,579 - INFO - âœ… Downloaded latest CSV: 20250601_234748_ai_tools_scraped.csv â†’ downloads\\20250601_234748_ai_tools_scraped.csv\n",
      "2025-06-02 14:54:44,364 - INFO - Columns dropped and null values dropped.\n",
      "2025-06-02 14:54:44,368 - INFO - Tags Column Successfully cleaned.\n",
      "2025-06-02 14:54:44,370 - INFO - Data successfully cleaned!\n",
      "2025-06-02 14:54:44,517 - INFO - Data successfully transformed!\n",
      "2025-06-02 14:54:46,000 - INFO - Database succesfully connected to.\n",
      "2025-06-02 14:54:46,368 - INFO - Existing DB Data and Scraped Data successfully merged!\n",
      "2025-06-02 14:54:46,999 - INFO - Database succesfully connected to.\n",
      "2025-06-02 14:55:12,474 - INFO - Data successfully loaded in database!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6986 entries, 0 to 6985\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   name          6986 non-null   object        \n",
      " 1   description   6986 non-null   object        \n",
      " 2   homepage_url  6986 non-null   object        \n",
      " 3   source        6986 non-null   object        \n",
      " 4   category      6986 non-null   object        \n",
      " 5   created_at    6986 non-null   datetime64[ns]\n",
      " 6   updated_at    6986 non-null   datetime64[ns]\n",
      " 7   trending      6986 non-null   object        \n",
      " 8   id            72 non-null     float64       \n",
      "dtypes: datetime64[ns](2), float64(1), object(6)\n",
      "memory usage: 491.3+ KB\n"
     ]
    }
   ],
   "source": [
    "ab = run_basic_etl()\n",
    "ab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c54986bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6986 entries, 0 to 6985\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   name          6986 non-null   object        \n",
      " 1   description   6986 non-null   object        \n",
      " 2   homepage_url  6986 non-null   object        \n",
      " 3   source        6986 non-null   object        \n",
      " 4   category      6986 non-null   object        \n",
      " 5   created_at    6986 non-null   datetime64[ns]\n",
      " 6   updated_at    6986 non-null   datetime64[ns]\n",
      " 7   trending      6986 non-null   bool          \n",
      " 8   id            72 non-null     float64       \n",
      "dtypes: bool(1), datetime64[ns](2), float64(1), object(5)\n",
      "memory usage: 443.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# scraped['created_at'] = datetime.now().strftime(\"%d/%m/%Y, %H:%M:%S\")\n",
    "ab.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-directory-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
