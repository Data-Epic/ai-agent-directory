{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675d500a",
   "metadata": {},
   "source": [
    "## TASKS\n",
    "```\n",
    "DESCRIPTION                             STATUS \n",
    "- Load seed data into DB.               done\n",
    "- Clean and transform scraped data      done\n",
    "- Create an idempotent etl job          ongoing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5008c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c588289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8b61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\29-05-2025_ai_tools_scraped.json\"\n",
    "seed_data_source = r\"C:\\Users\\APIN PC\\OneDrive\\Documents\\DS\\DE_Inter\\data_epic_capstone\\etl\\data\\seeded_ai_agents.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a81ac",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30430eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(source_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        source_path (str): Data Path\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raises error for unsupported data type.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas Dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        ext = Path(source_path).suffix\n",
    "        if ext == \".csv\":\n",
    "            return pd.read_csv(source_path)\n",
    "        elif ext == \".json\":\n",
    "            return pd.read_json(source_path)\n",
    "        elif ext == \".parquet\":\n",
    "            return pd.read_parquet(source_path)\n",
    "        logger.info(\"Data successfully read!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}. Unsupported file format! Use csv, json or parquet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a875aff",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69a1ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 378 entries, 0 to 377\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name         378 non-null    object\n",
      " 1   description  378 non-null    object\n",
      " 2   url          376 non-null    object\n",
      " 3   tags         377 non-null    object\n",
      " 4   pricing      377 non-null    object\n",
      " 5   page         378 non-null    int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 17.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72 entries, 0 to 71\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   name          72 non-null     object\n",
      " 1   description   72 non-null     object\n",
      " 2   homepage_url  72 non-null     object\n",
      " 3   category      72 non-null     object\n",
      " 4   source        72 non-null     object\n",
      " 5   created_at    72 non-null     object\n",
      " 6   updated_at    72 non-null     object\n",
      " 7   trending      72 non-null     object\n",
      "dtypes: object(8)\n",
      "memory usage: 4.6+ KB\n"
     ]
    }
   ],
   "source": [
    "scraped_df = read_data(scraped_data_source)\n",
    "seed_df = read_data(seed_data_source)\n",
    "\n",
    "scraped_df.info()\n",
    "seed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fa97d",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Cleaning \"tags\" column\n",
    "- separate the values in the list and choose the unique tag. \n",
    "- Each tag must be just a value. (i.e list of len 1)\n",
    "    + no # in value \n",
    "    + no duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b49919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = df.drop(columns=[col for col in ['pricing', 'page'] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        logger.info(\"Columns dropped and null values dropped.\",\n",
    "                    extra={\n",
    "                     \"Cols dropped\": ['pricing', 'page'],\n",
    "                     \"Null Values Dropped\": len(df) - len(new_df)\n",
    "                    }\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised {e}! Is the input a dataframe? Use a pandas dataframe.\",  exc_info=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_hashtags(tags):\n",
    "    try:\n",
    "        if isinstance(tags, list):\n",
    "            clean = [tag for tag in tags if '#' not in tag]\n",
    "        elif isinstance(tags, str):\n",
    "            clean = [tags] if \"#\" not in tags else []\n",
    "        else:\n",
    "            clean = []\n",
    "        clean = ','.join(clean)\n",
    "\n",
    "        if len(clean) < 4:\n",
    "            clean = clean.upper()\n",
    "        else:\n",
    "            clean = clean.lower().capitalize()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at tags column cleaning {e}! Use tags column.\",  exc_info=True)\n",
    "    return clean\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    try:\n",
    "        # df = baseline_cleaning(df=scraped_df)\n",
    "        df = df.drop(columns=[col for col in ['pricing', 'page'] if col in df.columns])\n",
    "        new_df = df.dropna()\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "        if 'tags' in df.columns:\n",
    "            new_df['tags'] = new_df['tags'].apply(remove_hashtags)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        logger.info(\"Columns dropped and null values dropped.\",\n",
    "                    extra={\n",
    "                     \"Cols dropped\": ['pricing', 'page'],\n",
    "                     \"Null Values Dropped\": len(df) - len(new_df)\n",
    "                    }\n",
    "                    )\n",
    "        logger.info(\"Tags Column Successfully cleaned.\")\n",
    "        logger.info(\"Data successfully cleaned!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at full cleaning process: {e}!\",  exc_info=True)\n",
    "    return new_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfcfc1",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3606b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_created_at(filepath: str) -> str:\n",
    "    try:\n",
    "        created_timestamp = os.path.getctime(filepath)\n",
    "        created_date = datetime.fromtimestamp(created_timestamp)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised: {e}!\", exc_info=True)\n",
    "    return created_date.strftime(\"%Y-%M-%d\")\n",
    "\n",
    "\n",
    "def transform_data(df: pd.DataFrame, source = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        created_day = get_created_at(scraped_data_source)\n",
    "        if 'source' in df.columns:\n",
    "            if df['source'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['source'] = source\n",
    "        else:\n",
    "            df['source'] = source\n",
    "\n",
    "\n",
    "        if 'created_at' in df.columns:\n",
    "            if df['created_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['created_at'] = created_day\n",
    "        else:\n",
    "            df['created_at'] = created_day\n",
    "        if 'updated_at' in df.columns:\n",
    "            if df['updated_at'] is not None:\n",
    "                pass\n",
    "            else:\n",
    "                df['updated_at'] = None\n",
    "        else:\n",
    "            df['updated_at'] = None\n",
    "        \n",
    "\n",
    "        if 'trending' not in df.columns:\n",
    "            df['trending'] = None\n",
    "            df['trending'] = df['trending'].notna().astype(bool)\n",
    "        else:\n",
    "            df[\"trending\"] = df[\"trending\"].apply(\n",
    "                lambda x: False if x == 'Low' else True\n",
    "                )\n",
    "            df['trending'] = df['trending'].astype(bool)\n",
    "    \n",
    "        trans_df = df.rename(columns={'url': 'homepage_url', 'tags': 'category'})\n",
    "        \n",
    "        trans_df['created_at'] = pd.to_datetime(trans_df['created_at'], format=\"%Y-%M-%d\", errors=\"coerce\")\n",
    "        trans_df['updated_at'] = pd.to_datetime(trans_df['updated_at'], format=\"%Y-%M-%d\", errors=\"coerce\")\n",
    "\n",
    "        logger.info(\"Data successfully transformed!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error Raised at transformation: {e}!\", exc_info=True)\n",
    "    return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba06c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_dfs(new_df, existing_df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merging DFs to extract unique ai_tools\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DF with unique Ai tools\n",
    "    \"\"\"\n",
    "    try:\n",
    "        merged_df = pd.merge(existing_df, new_df, how=\"outer\", suffixes=\"_existing\")\n",
    "        merged_df.drop_duplicates(subset=[\n",
    "            \"name\", \"homepage_url\"\n",
    "            ], inplace=True)\n",
    "        merged_df = merged_df.reset_index(drop=True)\n",
    "        logger.info(\"Existing DB Data and Scraped Data successfully merged!\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error merging DFs: %s\", e, exc_info=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f82458",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca5e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_etl() -> pd.DataFrame:\n",
    "    # Extract\n",
    "    scraped_df = read_data(scraped_data_source)\n",
    "\n",
    "    # Clean\n",
    "    clean_scraped_df = clean_data(scraped_df)\n",
    "\n",
    "    # Transform\n",
    "    trans_scraped_df = transform_data(\n",
    "        clean_scraped_df, source=\"https://aitoolsdirectory.com/\"\n",
    "    )\n",
    "\n",
    "    return trans_scraped_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6a3101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 12:23:45,994 - INFO - Columns dropped and null values dropped.\n",
      "2025-05-30 12:23:45,996 - INFO - Tags Column Successfully cleaned.\n",
      "2025-05-30 12:23:46,003 - INFO - Data successfully cleaned!\n",
      "2025-05-30 12:23:46,024 - INFO - Data successfully transformed!\n"
     ]
    }
   ],
   "source": [
    "comp_df = run_basic_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c39c5f",
   "metadata": {},
   "source": [
    "## Idempotent ETL Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75079cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ai_tools ETL Local DB Setup and data upload\n",
    "\n",
    "Name: Arowosegbe Victor Iyanuoluwa\\n\n",
    "Email: Iyanuvicky@gmail.com\\n\n",
    "GitHub: https://github.com/Iyanuvicky22/projects\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    Column,\n",
    "    String,\n",
    "    Text,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    "    func,\n",
    "    Integer,\n",
    ")\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "DB_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"\n",
    "    Database connector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(DB_URL)\n",
    "        Session = sessionmaker(bind=engine, autoflush=False)\n",
    "        Base.metadata.create_all(engine)\n",
    "        logger.info(\"Database succesfully connected to.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(\"Databse connection error: %s\", e, exc_info=True)\n",
    "    return Session, engine\n",
    "\n",
    "\n",
    "class Agent(Base):\n",
    "    \"\"\"\n",
    "    Agents table model creation\n",
    "    Args:\n",
    "        Base (): SQLAlchemy Base model\n",
    "    \"\"\"\n",
    "\n",
    "    __tablename__ = \"agents\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    name = Column(String, nullable=False, index=True)\n",
    "    description = Column(Text)\n",
    "    homepage_url = Column(String)\n",
    "    category = Column(String)\n",
    "    source = Column(String)\n",
    "    trending = Column(Boolean, default=False)\n",
    "    created_at = Column(DateTime, server_default=func.now(), nullable=False)\n",
    "    updated_at = Column(\n",
    "        DateTime, server_default=func.now(), onupdate=func.now(), nullable=False\n",
    "    )\n",
    "\n",
    "\n",
    "def load_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function to load data into the Database (PostgreSQL).\n",
    "    Args:\n",
    "        df (pd.DataFrame): Cleaned and Transformed data to be loaded.\n",
    "    \"\"\"\n",
    "    Session, engine = connect_db()\n",
    "    data = df\n",
    "\n",
    "    def change_to_none(row):\n",
    "        row.replace(pd.NaT, '')\n",
    "        return row\n",
    "\n",
    "\n",
    "    with Session.begin() as session:\n",
    "        for _, row in data.iterrows():\n",
    "            ai_tool = session.query(Agent).filter_by(name=str(row[\"name\"])).first()\n",
    "\n",
    "            if not ai_tool:\n",
    "                agent = Agent(\n",
    "                    name=str(row[\"name\"]),\n",
    "                    description=str(row[\"description\"]),\n",
    "                    homepage_url=row[\"homepage_url\"],\n",
    "                    category=row[\"category\"],\n",
    "                    source=row[\"source\"],\n",
    "                    trending=row[\"trending\"],\n",
    "                    created_at=row[\"created_at\"],\n",
    "                    updated_at=row[\"updated_at\"],\n",
    "                )\n",
    "                session.add(agent)\n",
    "        session.commit()\n",
    "        logger.info(\"Data successfully loaded in database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f49dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_load_seed_df():\n",
    "    \"\"\"\n",
    "    Function to Load Seed Data into Database\n",
    "    \"\"\"\n",
    "    data = read_data(source_path=seed_data_source)\n",
    "\n",
    "    trans_seed_df = transform_data(df=data)\n",
    "\n",
    "    load_data(trans_seed_df)\n",
    "\n",
    "    return trans_seed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadfc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_load_seed_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d23295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 13:27:17,157 - INFO - Database succesfully connected to.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "def fetch_db_records():\n",
    "    session, engine = connect_db()\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        db_df = pd.read_sql(\"SELECT * from agents\", con=conn)\n",
    "        conn.commit()\n",
    "    return db_df\n",
    "\n",
    "db_df = fetch_db_records()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4822601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trending\n",
       "True     60\n",
       "False    12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_df.trending.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d97d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc3718cf",
   "metadata": {},
   "source": [
    "## Functions to Work On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_check(new_df, existing_df):\n",
    "    # Merge and check for differences\n",
    "    merged = new_df.merge(\n",
    "        existing_df, on=[\"name\", \"homepage_url\"], how=\"left\", suffixes=(\"\", \"_existing\")\n",
    "    )\n",
    "    changed = merged[\n",
    "        (merged[\"name\"] != merged[\"name_existing\"])\n",
    "        | (merged[\"homepage_url\"] != merged[\"homepage_url_existing\"])\n",
    "    ]\n",
    "    return changed[new_df.columns] \n",
    "\n",
    "\n",
    "\n",
    "def upsert_records(conn, df):\n",
    "    cursor = conn.cursor()\n",
    "    for _, row in df.iterrows():\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO agents (name, homepage_url, email, phone)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "            ON CONFLICT(name, homepage_url)\n",
    "            DO UPDATE SET email=excluded.email, phone=excluded.phone\n",
    "        \"\"\",\n",
    "            (row[\"name\"], row[\"homepage_url\"], row[\"email\"], row[\"phone\"]),\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def etl_job(source_path):\n",
    "    df = read_data(source_path)\n",
    "\n",
    "    if needs_transformation(df):\n",
    "        df = transform_data(df)\n",
    "\n",
    "    with engine.connect(\"agents.db\") as conn:\n",
    "        existing_df = fetch_existing_records(conn)\n",
    "        delta_df = delta_check(df, existing_df)\n",
    "\n",
    "        if not delta_df.empty:\n",
    "            upsert_records(conn, delta_df)\n",
    "            print(f\"Upserted {len(delta_df)} records.\")\n",
    "        else:\n",
    "            print(\"No changes detected. Idempotent run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607bc10d",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Pending tasks\n",
    "- Load Seed data separately.    ```done```\n",
    "- Load scraped data and check for duplicates with name(lower) and homepage_url.\n",
    "- Write tests to check for:\n",
    "    + test for duplicates.\n",
    "    + test for invalid rows.\n",
    "    + test for correct upserts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agent-directory-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
